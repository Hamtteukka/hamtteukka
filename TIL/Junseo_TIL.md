# 1월 13일 TIL 

## 운영 체제

> 시스템 콜이 무엇인지 설명해 주세요.

운영체제는 커널 모드와 사용자 모드로 나뉘어 구동되며, 커널 모드에서만 메모리 등의 자원을 직접 조작하고, 하드웨어를 제어할 수 있기 때문에, 커널 모드로의 전환이 필요하다. 즉, 시스템 콜은 응용프로그램이 **커널이 제공하는 서비스를 이용하기 위한 인터페이스** 이다. 

**시스템콜은 커널 영역의 기능을 사용자 모드가 가용 가능하게, 즉 프로세스가 하드웨어에 접근해서 필요한 기능을 할 수 있게 해준다.**  

- 즉, 응용프로그램은 시스템 콜을 사용해서 원하는 기능을 수행할 수 있다.
- 보통 직접적으로 시스템 콜을 사용하기 보다는 API를 통해 사용하게 된다.

> 우리가 사용하는 시스템 콜의 예시를 들어주세요.

File I/O의 open, read, write, close 등이 있다. 예를 들어, read의 경우 하드웨어 메모리에 존재하는 특정 파일에 접근해서 파일을 읽을 준비를 하는 일련의 과정이 커널 모드에서 수행되게 된다. 이외에도 python의 sys, time 등의 함수 역시 시스템 콜의 예시로 들 수 있다. 

> 시스템 콜이, 운영체제에서 어떤 과정으로 실행되는지 설명해 주세요.

1. **사용자 프로그램 실행** 
    1. 먼저, 사용자가 작성한 프로그램이 사용자 모드에서 실행된다. 
2. **시스템 콜 호출**
    1. 프로그램이 운영체제의 특정 기능을 사용해야 할 때, 그 기능을 호출하는데 사용되는 함수 또는 명령을 호출한다. 이것이 바로 시스템 콜이다. 
        1. 예를 들어, 파일을 열고 읽는 작업을 해야 할 때, 사용자 프로그램은 파일 관련 시스템 콜을 호출한다. 
3. **사용자 모드에서 커널 모드로 전환**
    1. 시스템 콜 호출이 발생하면, 프로그램은 현재 실행 중인 사용자 모드에서 커널 모드로 전환된다. 커널 모드에서만 운영 체제의 핵심 기능과 하드웨어 자원에 접근할 수 있기 때문이다.
4. **요청 분석 및 처리**
    1. 커널은 내부적으로 시스템 콜 각각의 서비스 루틴에 대응되는 인덱스 테이블을 지니고 있고, 시스템 콜이 호출되면 이에 대응되는 인덱스를 참조하여 서비스 루틴을 수행한다. 
        1. 예를 들어, 파일 열기 요청의 경우 해당 파일을 찾아서 열기 작업을 수행한다. 
5. **작업 수행 및 결과 반환**
    1. 운영체제는 요청된 작업을 수행하고, 결과를 메모리에 저장하거나 레지스터에 반환한다. 
        1. 예를 들어, 파일을 읽는 작업의 경우 파일 내용을 메모리에 읽어온다. 
6. **커널 모드에서 사용자 모드로 전환**
    1. 요청된 작업이 완료되면, 운영체제는 다시 사용자 모드로 전환한다.
        1. 이제 프로그램은 작업 결과를 받아 사용자 모드에서 필요한 작업을 계속할 수 있다. 
7. **프로그램 실행 재개**
    1. 프로그램은 이제 시스템 콜 이후부터 다음 작업을 계속 수행한다. 필요한 결과를 받아 사용자 모드에서 다양한 작업을 진행할 수 있다.

> 시스템 콜의 유형에 대해 설명해 주세요.

**프로세스 제어**

- 프로세스 실행, 생성, 대기 등
- **종류**
    - fork() : 새로운 프로세스 생성
    - exec() : 새로운 프로그램 실행
    - exit() : 현재 프로세스 종료
    - wait() : 자식 프로세스가 종료될 때 까지 대기

**파일 조작**

- 파일 열기, 읽기, 쓰기 등
- **종류**
    - open() : 파일 열기
    - read()
    - write()
    - close()

**장치 관리**

- 디바이스 부착, 분리, 읽기, 쓰기 등
- **종류**
    - read() : 장치 읽기
    - write() : 장치 쓰기
    - ioctl() : 장치 제어

**정보 유지**

- 시간, 날짜 설정 등
- **종류**
    - getpid() : 현재 프로세스 ID 가져오기
    - alarm()  : 지정한 시간 후에 알람 시그널을 보내는 타이머를 설정
    - sleep() : 현재 프로세스를 지정한 동안 멈춤

**통신**

- 통신 연결 생성, 제거, 상태 정보 전달 등
- **종류**
    - pipe() : 두 프로세스 간에 단방향 통신 파이프를 생성
    - shm_open() : 공유 메모리 객체를 생성하거나 열기
    - mmap() : 파일의 내용을 메모리에 매핑하여 읽거나 쓸 수 있도록 변경

**보호**

- 파일 및 디렉터리의 접근 권한과 소유자 정보를 관리 등
- **종류**
    - chmod() : 파일 또는 디렉터리의 권한 변경
    - umask() : 새로운 파일 생성 시의 기본 권한 제한 설정
    - chown() : 파일 또는 디렉터리의 소유자와 그룹 소유자 변경


> 운영체제의 Dual Mode 에 대해 설명해 주세요.

Dual Mode는 OS를 보호하고 시스템 자원에 대한 안전한 접근을 보장하기 위해, **사용자 모드와 커널 모드로 나누는 기법**이다. 

사용자 프로세스는 제한된 권한을 가지며, 하드웨어 자원에 직접 접근할 수 없다. 자원에 접근하려면 시스템 콜을 통해 커널 프로세스에 요청해야 하며, 이 과정에서 **사용자 모드에서 커널 모드로 전환**이 이루어진다. 

**커널 모드**

운영체제의 핵심인 커널 코드가 실행 되는 상태이다. 커널은 운영체제의 핵심 기능을 수행하며 하드웨어 리소스에 접근하고 관리할 수 있는 권한을 가진다. 커널 모드에서 실행되는 코드는 시스템의 모든 자원에 접근하여 제어할 수 있으므로, 시스템 콜 및 운영체제의 서비스를 제공한다. 

**사용자 모드**

사용자 모드는 일반 응용 프로그램이 실행되는 상태이다. 이 모드에서 실행되는 프로그램은 자신의 주소 공간 내에서만 작동하며, 하드웨어 리소스에 직접적인 접근이 허용되지 않는다. 즉, 사용자 모드에서는 응용 프로그램이 제한된 권한을 가지며, 시스템 리소스에 대한 직접적인 제어는 불가능하다.

> 왜 유저모드와 커널모드를 구분해야 하나요?

사용자 프로세스가 항상 시스템 자원에 직접 접근 가능한 커널 모드에서 실행된다면, 시스템에 장애를 일으킬 수 있는 위험이 따른다. 이러한 위험을 방지하기 위해, 운영체제는 상대적으로 안전한 사용자 모드에서 프로그램을 실행한다. 

> 서로 다른 시스템 콜을 어떻게 구분할 수 있을까요?

통상적으로 시스템 콜은 여러 종류의 기능으로 나뉘어져 있으며, 각 시스템 콜에는 번호가 할당된다. **System call Interface는 이러한 번호에 따라 인덱스 테이블을 관리하고 서로 다른 시스템 콜을 구분**할 수 있다. 

일부 운영 체제에서는 지정된 함수의 이름이 운영체제 내부에서 시스템 콜 번호로 변환되어 호출하는 과정에서 시스템 콜을 구분하여 실행한다. 

위 두 방법 중 하나 또는 둘을 조합하여 시스템 콜을 구분하고 처리한다.

> 인터럽트가 무엇인지 설명해 주세요.

프로세스 실행 도중 예기치 않은 상황이 발생할 때, 발생한 상황을 처리한 후 실행중인 작업으로 복귀하는 것을 말한다. 

**내부 인터럽트**

- 하드웨어 고장
    - 컴퓨터 고장
    - 데이터 전달 과정에서의 비트 오류
    - 전원이 나간 경우
- 실행할 수 없는 명령어
    - 기억 장치에서 인출한 명령어의 비트 패턴이 정의되어 있지 않은 경우
- 명령어 실행 오류
    - 나누기 0을 하는 경우
- 사용 권한 위배
    - 사용자가 운영체제만 사용할 수 있는 자원에 액세스 하는 경우

**외부 인터럽트**

- 외부 인터럽트는 주로 **입출력장치**에 의해 발생된다.
    - **타이머 인터럽트**
        - 타이머가 일정한 시간 간격으로 중앙처리장치에게 인터럽트를 요청
    - **입출력 인터럽트**
        - 속도가 느린 입출력 장치가 입출력 준비가 완료되었음을 알리기 위해 인터럽트를 요청


> 인터럽트는 어떻게 처리하나요?

요청 → 중단 → 보관 → 인터럽트 처리 → 재개

- **인터럽트 요청**
- **프로그램 실행 중단**
    - 현재 실행 중이던 Micro Operation 까지 수행
- **현재 실행 중인 프로그램 상태 보관**
    - PCB (Process Control Block), PC (Program Counter) 저장
- **인터럽트 원인 판별**
    - 인터럽트를 요청한 장치를 식별 → 인터럽트 원인을 파악
    - Interrupt Vector 테이블을 참조하여 호출할 ISR (인터럽트 서비스 루틴) 주소 값을 얻음
- **ISR (인터럽트 서비스 루틴) 처리**
    - 실질적인 인터럽트 처리 작업을 한다.
    - 서비스 루틴 수행 중, 우선순위가 더 높은 인터럽트가 발생하면 재귀적으로 1 ~ 5 과정을 수행한다.
    - 인터럽트 서비스 루틴을 실행할 때 인터럽트 플래그를 0으로 하면 인터럽트 발생을 방지할 수 있다.
- **상태 복구**
    - 상태 복구 명령어가 실행되면, 저장해 둔 PC를 다시 복원하여 이전 실행 위치로 복원한다.
- **중단된 프로그램 실행 재개**
    - PCB의 값을 이용하여 이전에 수행 중이던 프로그램을 재개한다.


> Polling 방식에 대해 설명해 주세요.

CPU가 작업을 진행하다 입출력 명령을 만나면 직접 입출력 장치에서 데이터를 가져오는 방식이다. CPU가 직접 일을 하기 때문에 입출력을 하는 동안 다른 일은 못한다. 따라서 입출력이 처리되는 동안 기다려야 하는데 시간이 오래 걸리며 작업 효율이 떨어져 현재는 사용하지 않는다. 

또한 정해진 시간이나 주기적으로 체크하는 방식으로 과거에는 작업 개수가 적었을 때 가능했던 방식이 오늘날 다양한 프로세스 처리에는 알맞지 않다. 

- **인터럽트와의 비교**
    - 어떠한 주변 장치들의 입출력이나 하드웨어 문제, 프로그램에서 예외가 발생했을 때 CPU에게 이를 알려주는 방식.

> HW / SW 인터럽트에 대해 설명해 주세요.

**HW 인터럽트**

- 하드웨어 인터럽트는 주로 하드웨어 장치에서 발생하는 이벤트로, 외부에서 발생한 신호에 의해 컴퓨터 시스템의 CPU에 전달된다.
- 이러한 인터럽트는 하드웨어 장치의 상태 변화를 나타내며, 예를 들어 데이터 전송 완료, 타이머 만료, 키보드 입력 등이 해당한다.
- 하드웨어 인터럽트는 주로 입출력 장치와의 상호 작용에 사용되며, 장치의 상태 변화를 즉시 처리할 수 있도록 한다.

**SW 인터럽트**

- 소프트웨어 인터럽트는 주로 프로그램 내에서 발생하는 이벤트로, CPU가 현재 실행 중인 명령어에 의해 생성된다.
- 예를 들어, 프로그램이 시스템 콜을 호출하거나 예외상황이 발생할 때 소프트웨어 인터럽트가 발생한다.
- 소프트웨어 인터럽트는 주로 프로세스 관리, 예외 처리, 시스템 호출 등과 관련된 작업에 사용된다.

> 동시에 두 개 이상의 인터럽트가 발생하면, 어떻게 처리해야 하나요?

**인터럽트 우선 순위**

- 각 인터럽트에는 우선 순위가 할당되어 있고, 더 높은 우선순위를 갖는 인터럽트가 먼저 처리 된다.
- 이는 하드웨어나 소프트웨어에서 지정된 방식으로 우선 순위가 관리된다.
- 따라서 우선 순위가 높은 인터럽트가 먼저 처리 되고, 그 후에 낮은 우선 순위의 인터럽트가 처리 된다.

**인터럽트 마스킹**

- 시스템은 한 번에 하나의 인터럽트만 처리할 수 있기 때문에, 다중 인터럽트가 발생하면 다른 인터럽트를 일시적으로 마스크하여 처리 중인 인터럽트를 완료한 후에 처리할 수 있다.
- 이를 통해 다중 인터럽트에 대한 우선 순위나 순서를 관리할 수 있다.

**인터럽트 대기 큐**

- 시스템은 인터럽트를 대기하는 큐를 유지하여 동시에 발생한 인터럽트를 처리하는 방식이다.
- 인터럽트가 발생하면 해당 인터럽트를 큐에 추가하고, 우선 순위에 따라 큐에서 처리된다.
- 이 방식은 다중 인터럽트를 효율적으로 처리할 수 있도록 한다.


## CKA
<aside>
❗**YAML 말고 명령어로 pod 만들기**

</aside>

```
kubectl run nginx --image=nginx
```

<aside>
❗**명령어로 pods가 위치되어있는 nodes 보기**

</aside>

describe 해서 봐도 된된다

```
kubectl get pods -o wide
```

하면 쭉 볼 수 있음 

<aside>
❗**컨테이너 갯수 보기**

</aside>

```
kubectl get pods
```

해서 READY에 있는 뒤에 있는 수가 컨테이너 갯수

<aside>
❗**READY 컬럼이 의미하는 것**

</aside>

Running containers in pod / total containers in pod

<aside>
❗**yaml 파일로 생성하는 쉬운 방법**

</aside>

```
kubectl run redis --image=redis123 --dry-run=client -o yaml
```

- **`-dry-run=client`**:
- 실제로 리소스를 클러스터에 적용하지 않고, 생성될 리소스의 YAML만 출력합니다.
- 리소스가 올바르게 생성될지 시뮬레이션하는 데 사용됩니다.

<aside>
❗**pod 수정하기**

</aside>

```
kubectl edit
```

```
cat redis.yml
vi redis.yml
```


<hr>

# 0114 TIL


## 화상 회의를 구현하는 방법 ( 1 : 1 )

[[WebRTC] 화상 회의를 구현하는 방법 (1:1)](https://hwanheejung.tistory.com/47)

### 프롤로그

채팅을 구현하기 위해서는 지속적인 연결을 유지하는 웹 소켓을 사용한다. 

웹 소켓에서는 A가 B에게 채팅을 보낼 때 두 사용자가 직접 연결되어 있는 것이 아니라, 서버의 중개를 통해 대화를 나눈다.

즉, A가 채팅을 보내면, 서버로 전송되고, 서버는 해당 채팅방을 구독하고 있는 사용자들에게 메시지를 보내주는 것이다. 이 방식은 클라이언트의 수가 늘어날 수록 서버에게 큰 부담이 된다. 간단한 텍스트를 보낼 때는 오버헤드가 작을 수도 있지만, 주고 받는 데이터가 영상이나 오디오처럼 용량이 크다면 서버의 부담이 엄청날 것이다. 

Web RTC는 서버의 중개 없이 클라이언트끼리 직접 소통한다. 이로 인해 서버는 데이터를 처리할 필요가 없어지고, 클라이언트끼리 직접 소통하다 보니 빠르게 소통할 수 있게된다. 

### WebRTC 동작 원리

1. **Signaling**
    1. 두 피어가 서로 통신을 시작하기 전에 서로의 정보를 교환할 필요가 있다. 서버가 이를 중계해주는데, 이를 signaling server라고 한다. 
    2. 즉, 브라우저는 서로가 무엇으로 어떻게 소통할 수 있는지 signaling server를 통해 정보를 교환한다. 
    3. signaling 자체는 WebRTC의 일부가 아니기 때문에 웹소켓이나 HTTP 등 외부 protocol을 사용해서 구현해야 한다. 
2. **ICE**
    1. 네트워크 경로를 설정하기 위해 사용되는 protocol
    2. 여러 종류의 candidate를 사용해서 연결을 시도한다. 각 ICE  Candidate에는 클라이언트들이 각각 소통에 사용할 수 있는 네트워크 경로들의 정보가 담겨 있다. 
    3. **Host Candidate**
        1. 같은 로컬 네트워크에 속한 경우이며, 각 기기의 네트워크 내부용 주소인 private IP 주소와 포트 번호를 교환한다. 
    4. **Server Reflexive Candidate**
        1. NAT 뒤에 있는 경우이며, STUN 서버를 사용하여 public IP 주소와 포트 번호를 알아내어 ICE Candidate으로 등록한다. 
        - **NAT**
            
            내부 네트워크의 private IP 주소를 public IP 주소로 변환하는 방법으로, NAT를 통해 외부 네트워크와 통신할 수 있게 된다. 
            
        - **STUN (Session Traversal Utilities for NAT)**
            
            상대방에게 알려줄 나의 주소를 알아내기 위해 STUN 서버를 사용한다. NAT 뒤에 있는 기기가 STUN 서버에 요청을 보내면, 응답으로 public IP 주소와 포트 번호를 알려준다. 
            
    5. **Relay Candidate**
        1. 네트워크 환경이 복잡하거나 방화벽 등에 의해 P2P 통신이 제한된 경우 TURN 서버를 사용한다. 이때 추가적인 경로를 거치므로 지연 시간 증가와 비용이 발생한다. 
        - **TURN 서버**
            
            만약 STUN 서버를 통해서도 직접 연결이 불가능하면, 클라이언트는 TURN 서버에 자신을 등록하고, 데이터 전송 시 TURN 서버가 각 클라이언트로 데이터를 중계한다. 
            
        
3. **SDP**
    1. Peer 간의 연결 설정을 위해 사용하는 protocol로, ICE Candidate 정보를 SDP에 포함 시켜서 상대방에게 전달한다. 



## 다대다 화상회의: OpenVidu를 도입하기까지의 자료조사

[[WebRTC] 다대다 화상회의: OpenVidu를 도입하기까지의 자료조사](https://hwanheejung.tistory.com/48)



### 연결이 수립되기까지의 Flow



크게 **SDP Offer/Answer 과정**과 **ICE Candidate 교환 과정**으로 나눌 수 있다. 이 두 과정은 독립적, 병렬 적으로 일어난다.  그러니까 SDP offer/answer 과정이 진행되는 동안에도 ICE Candidate 정보를 교환해서 빠르게 P2P 연결이 수립된다. 

**SDP Offer / Answer 과정 (미디어 정보 교환하자!)**

1. Peer A는 카메라, 마이크 상태와 같은 **Media 정보를 등록**
2. 이 media 정보를 SDP에 담아 signaling server를 통해 Peer B에게 전달 - **Offer**
3. offer를 받은 Peer B는 마찬가지로 자신의 Media 정보를 **Answer**로 응답 


**ICE Candidate 교환 과정 ( 너 누구야 ? )**

1. Peer A는 STUN 서버 또는 TURN 서버를 통해 자신의 IP 주소와 포트 정보를 알아낸 후, ICE candidate 객체를 포함시켜 signaling server을 통해 Peer B에게 전달한다.
2. Peer B도 마찬가지로 자신의 ICE candidate 정보를 signaling server를 통해 Peer A에게 전달한다.
3. Peer A와 Peer B는 각각 받은 ICE candidate 정보를 이용하여 직접 연결을 시도한다. 
4. 연결 성공


**Signaling Server의 역할**

브라우저끼리 통신을 하려면 서로 누군지 알아야 하는데, 그 과정을 signaling server가 중계해주는 것이다. 즉, Signaling server는 단순히 offer, answer, 그리고 ICE candidate **정보를 교환하는 중계 역할만 하고 이 과정이 끝나면 더 이상 관여하지 않고 브라우저끼리 소통하게 된다.** 


## 다대다 화상회의는 ?

**Mesh**

모든 참가자가 서로 직접 연결된다면 (Mesh 방식) 몇십 명, 몇백 명 정도를 감당하기에는 무리가 있어 보인다. 간단하게 생각해 봐도, 4명이 화상회의를 한다고 했을 때, A-B, A-C, A-D, B-C, C-D 즉 6번의 연결 수립이 이루어져야 한다. 참여자가 늘어날수록 연결의 수는 기하급수적으로 늘어나게 된다. 

즉, 다대다 통신을 P2P 통신으로 구현할 경우 클라이언트에 큰 부담이 될 것이다. 그래서 나온 것이 SFU, MCU 방식의 **media server** 이다. 

**SFU (Selective forwarding Unit)**

5명의 참가자가 있다고 했을 때, A는 자신의 영상 데이터를 B,C,D,E 에게 모두 전송하는 것이 아니라 MEDIA 서버에만 전송한다. Media 서버는 A의 데이터를 B,C,D,E에게 뿌린다. 즉 1명당 1개의 uplink (클라 → 서버) , 4개의 downlink (서버 → 클라) 를 가지게 된다. 

즉, 브라우저끼리의 P2P가 아니라 서버-클라이언트 간의 peer 연결이다. Mesh 방식과 비교하면 downlink의 수는 N-1로 같지만, uplink의 값은 1개로 줄었다. 

일부 참가자는 uplink가 필요 없을 수도 있다. 라이브 방송과 같이 publisher가 1명, subscriber가 N명인 1:N 통신인 경우, publisher는 영상 데이터를 media server에 한 번만 업로드하고 N명의 subscriber는 서버로부터 수신하는 경우 SFU를 사용하면 연결 수가 확실히 줄어든다. 

또는 회의에서 발표하는 그룹과 듣는 그룹이 정해져 있는 경우와 같이 데이터를 송출하는 그룹(N명)과 수신하는 그룹(M명)이 명확하게 분리되어 있는 경우도 SFU 방식이 적합하다. 

**MCU (Multipoint Control Unit)**

MCU에서는 Media 서버가 모든 영상 데이터를 받아서 하나로 합치고, 이를 모든 클라이언트로 전송한다. 예를 들어 5명의 화상회의에서 중앙 서버는 A를 제외한 4명의 영상, 음성 데이터를 하나로 합쳐서 A에게 보낸다. 

나머지 참가자들에게도 동일하게 적용되어 1명당 1개의 uplink, 1개의 downlink를 가지게 된다. 이 방식은 서버에 더 많은 부하를 주지만 클라이언트는 하나의 스트림만 전송, 수신하므로 부하가 확실히 줄어든다. 

> **하지만 애초에 WebRTC가 나온 이유가 서버의 부하를 줄이고 실시간성을 높이기 위함인데, MCU 방식은 일반적인 서버 기반 통신과 다를 것이 없는게 아닐까?**
> 

아니다. WebRTC의 MCU 방식에서도 서버에 부하가 걸리지만, 프로토콜 최적화와 QoS 관리, 그리고 보안 측면 덕분에 더 효율적으로 관리할 수 있다. 서버에 부하가 크긴 하지만, 그래도 WebRTC를 안 쓰는 것보단 쓰는게 훨씬 더 효율적이다. 

**Media Server가 하는 일** 

1. **Group Communications**
    1. 위에서 설명한 것처럼 한 peer가 생성한 미디어 스트림을 여러 subscriber들에게 전달한다. SFU, MCU 역할을 하는 것이다. 
2. **Mixing**
    1. 여러 개로 들어오는 미디어 스트림을 하나의 화면에 합성해서 각 참가자가 N개 비디오 화면을 동시에 볼 수 있게 하고, 여러 참가자의 목소리를 하나의 오디오 스트림으로 mixing 해서 동시에 들을 수 있게 하는 것을 말한다. 
3. **Transcoding**
    1. 클라이언트 간에 호환이 되지 않을 때, codec과 format을 실시간으로 변환하는 작업을 말한다. 예를 들어 한 클라이언트가 H.264 비디오 codec을 사용하고 다른 클라이언트가 VP8 비디오 codec을 사용한다면, 서버는 H.264 스트림을 VP8로 변환하여 호환성을 유지한다. 
4. **Recording**
    1. 클라이언트 간에 교환되는 미디어를 저장한다. 예를 들어 회의를 녹화해서 나중에 또 볼 수 있게 한다. 

---


## Kurento

media server의 일종. 무료이다. docs가 정말 자세하다. MCU, SFU를 모두 지원하며, 클라이언트 API를 제공한다. 

[Introduction to Kurento — Kurento 7.1 documentation](https://doc-kurento.readthedocs.io/en/latest/user/intro.html#why-a-webrtc-media-server)

**Distribution of Media and Application Services**

Kurento Media Server(KMS)는 여러 기기에서 배포, 확장, 분산될 수 있다. 하나의 애플리케이션이 여러 KMS를 호출할 수 있고, 반대로 하나의 KMS도 여러 애플리케이션의 요청을 처리할 수 있다.

**Application development**

개발자는 복잡한 KMS 내부를 신경 쓰지 않아도 된다. 모든 프레임워크에서 배포 가능하다.

**End-to-End Communication Capability**

Kurento는 미디어의 전송, encoding/decoding 및 클라이언트 장치에서의 렌더링 복잡성을 처리하지 않아도 되는 end-to-end 통신을 제공한다.

그러니까 개발자들이 WebRTC 기반의 SFU, MCU를 low level에서 개발하지 않아도 kurento를 사용해서 어느 정도 쉽게 구현할 수 있고, 추가적으로 다양한 기능들도 많이 지원한다.

그런데 kurento docs에 눈에 띄는 경고문이 있다.

!https://blog.kakaocdn.net/dn/GA1RI/btsIGl2wXTi/5iHri7xK5GFfWebDxRm3J0/img.png

초짜면 Openvidu를 써라는 것이다. 

## OpenVidu

Kurento 기반의 프레임워크이다. WebRTC를 사용하는 경우는 보통 화상 회의를 구현해야 하는 경우일텐데, 간단한 화상 회의를 구현해야 하는데 굳이 Kurento의 low level 코드를 만지지 않아도 된다. 

복잡한 기능을 개발해야 한다면 kurento를 써도 되는데, openvidu로도 충분히 많은 기능을 구현할 수 있기 때문에 kurento 공식 페이지에서도 openvidu를 추천한다. 

OpenVidu 페이지로 가봤더니 v3.0.0에 엄청난 변화가 생겼다. 내부 media server로 kurento를 버리고 mediasoup를 선택했고, livekit가 도입됐다. 하지만 문제는 Mediasoup는 node.js 환경에서 사용되도록 설계되었는데 우리는 spring을 사용하고 있다. 통합을 시도해 봤지만 쉽지 않았고, 그래서 kurento 기반의 v2.30.0 버전을 쓰기로 했다.


<hr>

# 0115 TIL

### OS

> 프로세스가 무엇인가요?


실행 중인 프로그램을 의미한다. 프로세스는 운영체제에 의해 관리되며, 독립적으로 실행되고 자원을 할당 받을 수 있는 단위이다. 운영체제는 프로세스들에게 적절히 자원들을 분배하여 여러가지 작업을 수행할 수 있게 한다. 


> 프로그램과 프로세스, 스레드의 차이에 대해 설명해 주세요.

[👩‍💻 ‍완전히 정복하는 프로세스 vs 스레드 개념](https://inpa.tistory.com/entry/%F0%9F%91%A9%E2%80%8D%F0%9F%92%BB-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%E2%9A%94%EF%B8%8F-%EC%93%B0%EB%A0%88%EB%93%9C-%EC%B0%A8%EC%9D%B4#%EC%A0%95%EC%A0%81_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8_static_program)

프로세스는 실행 중인 프로그램을 의미하고, 독립된 메모리 공간을 가진다. 반면, 스레드는 프로세스 내에서 실행되는 실행 단위로, 프로세스의 자원을 공유하면서 동작한다. 

**프로세스**는 운영체제로부터 자원을 할당받은 **작업의 단위**이다. 스레드는 할당받은 자원을 이용하는 **실행 흐름의 단위**이다. 

**정적 프로그램**

**컴퓨터에서 실행 할 수 있는 파일**을 통칭한다. 단, 아직 **파일을 실행하지 않은 상태**이기 때문에 정적 프로그램을 줄여서 프로그램이라고 부른다. 즉, **코드 덩어리** 이다.

**프로세스**

**프로그램이 돌아가고 있는 상태**를 말한다. 즉, 컴퓨터에서 **작업** 중인 프로그램을 의미한다. 

| **프로그램** | **프로세스** |
| --- | --- |
| 어떤 작업을 하기 위해 실행할 수 있는 파일 | 실행되어 작업중인 컴퓨터 프로그램 |
| 파일이 저장 장치에 있지만 메모리에는 올라가 있지 않은 정적인 상태  | 메모리에 적재되고 CPU  자원을 할당받아 프로그램이 실행되고 있는 상태  |
| 쉽게 말해 그냥 코드 덩어리 | 그 코드 덩어리를 실행한 것 |

**스레드**

하나의 **프로세스 내에서 동시에 진행되는 작업 갈래, 흐름의 단위**를 말한다. 

- 비유
    
    크롬 브라우저가 실행되면 프로세스 하나가 생성된다. 그런데 우리는 브라우저에서 파일을 다운 받으며 온라인 쇼핑을 하면서 게임을 하기도 한다. 
    

즉, 하나의 프로세스 안에서 여러 작업들 흐름이 동시에 진행되기 때문에 가능한 것인데, 이러한 일련의 작업 흐름들을 스레드라고 하며 여러개가 있다면 이를 다중 스레드 라고 부른다. 

스레드 수가 많을 수록 당연히 프로그램 속도도 동시에 하는 작업이 많아져 성능이 올라간다. 

일반적으로 하나의 프로그램은 하나 이상의 프로세스를 가지고 있고, 하나의 프로세스는 반드시 하나 이상의 스레드를 갖는다. 즉, 프로세스를 생성하면 기본적으로 하나의 main 스레드가 생성된다. 

- 추가
    
    ### 프로세스 & 스레드의 메모리
    
    ---
    
    **프로세스의 자원 구조** 
    
    프로그램이 실행되어 프로세스가 만들어지면 다음 4가지의 메모리 영역으로 구성되어 할당받게 된다. 
    
    **코드 영역**
    
    프로그래머가 작성한 프로그램 함수들의 코드가 CPU가 해석 가능한 기계어 형태로 저장되어 있다. 
    
    **데이터 영역**
    
    코드가 실행되면서 사용하는 전역 변수나 각종 데이터 들이 모여있다. 데이터 영역은 .data, .rodata, .bss 영역으로 세분화 된다. 
    
    **스택 영역**
    
    지역 변수와 같은 호출한 함수가 종료되면 되돌아올 임시적인 자료를 저장하는 독립적인 공간이다. Stack은 함수의 호출과 함께 할당되며, 함수의 호출이 완료되면 소멸한다. stack 영역을 초과하면 stack overflow 에러가 발생한다. 
    
    **힙 영역**
    
    생성자, 인스턴스와 같은 동적으로 할당되는 데이터들을 위해 존재하는 공간이다. 사용자에 의해 메모리 공간이 동적으로 할당되고 해제된다. 
    
    코드와 데이터 영역은 선언할 때 그 크기가 결정되는 정적 영역이지만, 스택 영역과 힙영역은 프로세스가 실행되는 동안 크기가 늘어났다 줄어들기도 하는 동적 영역이다.
    
    **스레드의 자원 공유** 
    
    프로세스의 자원을 공유하면서 프로세스 실행 흐름의 일부가 된다. **스레드는 Stack만 할당 받아 복사하고 Code, Data, Heap은 프로세스 내의 다른 스레드 들과 공유**된다. 
    
    따라서 각각의 스레드는 별도의 stack을 가지고 있지만 heap 메모리는 고유하기 때문에 서로 다른 스레드에서 가져와 읽고 쓸 수 있게 된다. 
    
    
    ```
    stack은 함수 호출 시 전달되는 인자, 되돌아갈 주소값, 함수 내에서 선언하는 변수 등을 저장하는 메모리 공간이기 때문에, 독립적인 스택을 가졌다는 것은 독립적인 함수 호출이 가능하다 라는 의미이다. 그리고 독립적인 함수 호출이 가능하다는 것은 독립적인 실행 흐름이 추가된다는 말이다.
    즉, stack을 가짐으로써 스레드는 독립적인 실행 흐름을 가질 수 있게 되는 것이다.
    ```
    
    ```
    반면에 프로세스는 기본적으로 프로세스 끼리 다른 프로세스의 메모리에 직접 접근할 수는 없다.
    ```
    


> **PCB가 무엇인가요?**


운영체제에서 프로세스를 관리하기 위해 해당 프로세스의 상태 정보를 담고 있는 자료구조를 말한다. 

프로세스를 컨텍스트 스위칭 할 때 기존 프로세스의 상태를 어딘가에 저장해 둬야 다음에 똑같은 작업을 이어서 할 수 있을 것이고, 새로 해야 할 작업의 상태 또한 알아야 어디서부터 다시 작업을 시작할 지 결정할 수 있을 것이다. 즉. PCB는 프로세스 스케쥴링을 위해 프로세스에 관한 모든 정보를 저장하는 임시 저장소 이다. 

프로세스가 생성되면 **메모리에 해당 프로세스의 PCB가 함께 생성, 종료 시 삭제** 


> **그렇다면, 스레드는 PCB를 갖고 있을까요?**


PCB 처럼, TCB(스레드 제어 블록)이 있다. 각 스레드마다 운영체제에서 유지하는 스레드에 대한 정보를 담고 있는 자료구조이다. TCB는 PCB 안에 들어있다. 스레드의 상태 정보, 스레드 ID, 스레드 우선순위, 스케쥴링 정보 등 다양한 정보를 저장한다. TCB도 스레드가 생성될 때 운영 체제에 의해 생성되며, 스레드가 실행을 마치고 소멸 될 때 함께 소멸된다. 


> **리눅스에서, 프로세스와 스레드는 각각 어떻게 생성될까요?**


프로세스는 시스템 호출인 fork()를 통하여 생성이 됩니다. 또한 리눅스는 최초의 프로세스인 init을 갖고 있습니다. 스레드는 프로세스와 비슷한 양상으로 시스템 호출인 pthread_create()를 사용하여 새로운 스레드를 생성합니다.


> 자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽게 되면 어떻게 처리하나요?

**[ 고아 프로세스 ]**

부모 프로세스가 자식 프로세스보다 먼저 종료되면 init 프로세스가 자식 프로세스의 새로운 부모 프로세스가 된다. 종료되는 프로세스가 발생할 때 커널은 이 프로세스가 누구의 부모 프로세스인지 확인한 후, 커널이 자식 프로세스의 부모 프로세스 ID를 1 (INIT 프로세스)로 바꿔준다. 

**[ 좀비 프로세스 ]**

자식 프로세스가 부모 프로세스 보다 먼저 종료되는 경우이다. 자식 프로세스가 EXIT 시스템 콜을 호출하면서 종료하면 이 프로세스에 관련된 모든 메모리와 리소스가 해제되어 다른 프로세스에서 사용할 수 있게 된다. 

자식 프로세스가 종료된 이후에 부모 프로세스가 자식 프로세스의 상태를 알고 싶을 수 있기 때문에 커널은 자식 프로세스가 종료되더라도 최소한의 정보를 가지고 있게 된다. 

최소한의 정보 : 프로세스 ID, 프로세스 종료 상태 등 


> **리눅스에서, 데몬프로세스에 대해 설명해 주세요.**


[데몬(daemon) 프로세스의 정의와 실행 방법](https://wildeveloperetrain.tistory.com/168)

**데몬(Daemon)**

정해진 일을 하는 프로세스다. 

Unix 운영체제에서 부팅 시 자동으로 켜져, 백그라운드에서 계속 실행되는 프로세스이다. 

데몬은 백그라운드 프로세스에 속하기 때문에 백그라운드 프로세스와 마찬가지로 TTY(터미널 장치)를 가지고 있지 않으며, 추가로 PPID(parent id)가 1이고, SID(session id) 역시 자신의 아이디와 같다는 특징이 있다. 

유닉스 (리눅스를 포함한) 운영체제에서 이름이 d로 끝나는 프로세스들이 대표적인 데몬 프로세스라고 볼 수 있다

ex) initd, httpd, nfsd, sshd, lpd, systemd, ftpd, syslogd 등이 있습니다.


> **리눅스는 프로세스가 일종의 트리를 형성하고 있습니다. 이 트리의 루트 노드에 위치하는 프로세스에 대해 설명해 주세요.**


리눅스에서 프로세스 트리의 루트 노드는 init 프로세스이다. 

init 프로세스는 시스템 부팅 시에 가장 먼저 실행되는 프로세스로, 나머지 모든 프로세스는 init의 자손이거나 자식 프로세스라고 볼 수 있다. 



> **프로세스 주소공간에 대해 설명해 주세요.**


**코드 영역**

프로그래머가 작성한 프로그램 함수들의 코드가 CPU가 해석 가능한 기계어 형태로 저장되어 있다. 

**데이터 영역**

코드가 실행되면서 사용하는 전역 변수나 각종 데이터 들이 모여있다. 데이터 영역은 .data, .rodata, .bss 영역으로 세분화 된다. 

**스택 영역**

지역 변수와 같은 호출한 함수가 종료되면 되돌아올 임시적인 자료를 저장하는 독립적인 공간이다. Stack은 함수의 호출과 함께 할당되며, 함수의 호출이 완료되면 소멸한다. stack 영역을 초과하면 stack overflow 에러가 발생한다. 

- 지역 변수와 매개변수가 저장되는 영
- 메모리의 높은 주소에서 낮은 주소의 방향으로 할당된다.

**힙 영역**

생성자, 인스턴스와 같은 동적으로 할당되는 데이터들을 위해 존재하는 공간이다. 사용자에 의해 메모리 공간이 동적으로 할당되고 해제된다. 

- 런타임에 크기가 결정되는 영역이다.
- 메모리의 낮은 주소에서 높은 주소의 방향으로 할당된다.

코드와 데이터 영역은 선언할 때 그 크기가 결정되는 정적 영역이지만, 스택 영역과 힙영역은 프로세스가 실행되는 동안 크기가 늘어났다 줄어들기도 하는 동적 영역이다.


> **초기화 하지 않은 변수들은 어디에 저장될까요?**


전역변수랑 정적 변수가 초기화 되지 않았을 때 Data 영역에 BSS 세그먼트에 저장된다.

지역변수는 처리하는 함수의 스택에 저장된다. 



> **일반적인 주소공간 그림처럼, Stack과 Heap의 크기는 매우 크다고 할 수 있을까요? 그렇지 않다면, 그 크기는 언제 결정될까요?**


- 스택과 힙 모두 디폴트 크기 값이 존재하며, 이는 **운영체제와 컴파일러에 따라 다를 수 있음**
- 스택의 경우, 윈도우에서는 1MB, 리눅스에서는 8MB가 기본 크기
- 힙의 경우, 1MB 또는 더 작은 크기를 기본 크기값부터 시작해, 동적 메모리 할당시 크기가 점점 증가함
- 힙의 크기는 물리적 리소스 또는 가용한 가상 메모리 주소 공간에 의해 제한됩니다. 즉, 이론적으로 32비트의 경우 2~3GB까지 제한되며, 64비트의 경우 거의 제한이 없다(16엑사바이트까지)고 할 수 있음

스택 영역의 크기는 컴파일 타임에 결정됩니다. 함수 호출이 발생하면 스택 프레임이 생성되어 스택 영역이 증가하고, 함수가 종료되면 스택 프레임이 제거되어 스택 영역이 감소합니다. 스택 오버플로우는 스택 영역의 크기가 프로세스에 할당된 스택 영역의 최대 크기를 초과할 때 발생합니다.

반면에 힙 영역의 크기는 런타임에 결정됩니다. 프로그램이 동적으로 메모리를 할당하거나 해제할 때마다 힙 영역의 크기가 변화합니다. 메모리 누수는 프로그램이 할당한 메모리를 제대로 해제하지 않아 힙 영역이 점차 증가하는 현상을 의미합니다.



> **Stack과 Heap 공간에 대해, 접근 속도가 더 빠른 공간은 어디일까요?**


Stack 공간의 접근 속도가 Heap 공간보다 더 빠릅니다.
Stack은 정적 메모리 할당을 사용하며, 데이터가 연속된 메모리 블록에 저장되기 때문에 접근 속도가 빠릅니다.
반면, Heap은 동적 메모리 할당을 사용하며, 메모리 블록이 불연속적으로 분포되어 있어 접근 속도가 느립니다

스택 메모리는 LIFO(Last In First Out) 방식으로 관리되며, 메모리 할당과 해제가 상대적으로 빠릅니다. 스택 프레임이 생성되고 제거되는 것은 단순히 스택 포인터를 이동시키는 것이기 때문에, 이는 매우 빠른 연산입니다. 그러나 스택의 크기는 컴파일 타임에 결정되어 실행 중에는 변경할 수 없습니다.

반면에 힙 메모리는 동적으로 할당되고 해제되며, 메모리 관리가 복잡하고 시간이 더 많이 소요됩니다. 메모리 블록을 찾고, 할당하고, 해제하는 데 필요한 오버헤드가 있기 때문입니다. 그러나 힙의 크기는 유연하게 조절할 수 있어 필요에 따라 큰 데이터를 저장하는 데 사용할 수 있습니다.

따라서, 일반적으로 스택의 메모리 접근 속도는 힙보다 빠르다고 할 수 있지만, 실제 성능은 메모리를 어떻게 사용하느냐에 따라 크게 달라질 수 있습니다.

스택 할당이 힙할당보다 한참 싸고, 접근할때 인스트럭션도 덜 들어가고, 캐시히트 측면에서도 유리하다



> **다음과 같이 공간을 분할하는 이유가 있을까요?**


메모리를 분할하여 관리함으로써 각 영역에 맞는 메모리 관리 전략을 사용할 수 있고, 프로그램의 안정성과 효율성을 높일 수 있다.


> **스레드의 주소공간은 어떻게 구성되어 있을까요?**


각 스레드는 고유한 스택 영역을 가진다. 이 외에도 각 스레드는 **스레드 로컬 스토리지(TLS)**라는 고유한 데이터 영역을 가질 수 있다.  이는 각 스레드가 자신만의 전역 변수를 가질 수 있게 한다. 

그러나 코드영역, 데이터 영역, 그리고 힙 영역은 프로세스 내의 모든 스레드가 공유한다.

스레드의 주소 공간은 일부를 프로세스와 공유하면서도, 스레드마다 고유한 스택 영역과 TLS를 가지는 형태로 구성되어 있다. 이런 구조는 스레드가 독립적인 실행 흐름을 가질 수 있게 해주면서도, 필요한 데이터와 코드를 효율적 공유할 있게한다. 



> **"스택"영역과 "힙"영역은 정말 자료구조의 스택/힙과 연관이 있는 걸까요? 만약 그렇다면, 각 주소공간의 동작과정과 연계해서 설명해 주세요.**


스택 메모리 영역과 힙 메모리 영역은 자료구조와는 몇 가지 공통점이 있지만, 그렇다고 완전히 같은 개념은 아니다.

**스택 영역** : 이 영역은 자료구조의 스택처럼 후입선출 방식으로 동작한다. 함수가 호출되면, 해당 함수의 지역 변수와 매개변수, 반환 주소 등이 스택에 PUSH 되어 쌓이고, 함수가 종료되면 그 정보는 POP되어 제거된다. 

**힙 영역** : 이 영역은 자료구조의 힙과는 다르게 동작한다. 자료구조의 힙은 자료의 우선순위에 따라 정렬되는 반면, 메모리의 힙 영역은 동적으로 메모리를 할당하고 해제하는 역할을 한다. 프로그래머가 필요한 만큼 메모리를 할당하고 사용이 끝나면 해제할 수 있다. 이 과정은 자료구조의 힙과 직접적인 연관이 없다. 


> **IPC의 Shared Memory 기법은 프로세스 주소공간의 어디에 들어가나요? 그런 이유가 있을까요?**


[[운영체제] 멀티 프로세스 환경에서 프로세스간 데이터는 어떻게 전달될까?](https://hyuuny.tistory.com/153)

[[Process & Thread] 프로세스간 통신(IPC, 공유메모리, 메시지전달)](https://karla.tistory.com/133)

별도의 공유 메모리가 생성된다. 


> **스택과 힙영역의 크기는 언제 결정되나요? 프로그램 개발자가 아닌, 사용자가 이 공간의 크기를 수정할 수 있나요?**


**스택 영역의 크기**는 컴파일 때 결정되며, 운영체제나 컴파일러 설정에 따라 크기가 달라질 수 있다. 일반적으로 스택의 크기는 고정되어 있고 이 크기를 초과하면 ‘스택 오버플로우’라는 오류가 발생한다. 

**힙 영역의 크기**는 프로그램이 실행되는 동안 동적으로 변한다. 프로그램이 메모리를 필요로 할 때 운영체제에 요청하여 힙 영역에서 메모리를 할당 받는다. 사용이 끝난 메모리는 프로그램에 의해 해제 되어 다시 힙으로 반환된다. 

일반적으로 프로그램 사용자가 직접 스택과 힙의 크기를 수정하는 것은 불가능하다. 이들 영역의 크기는 운영체제의 메모리 관리 정책과 프로그램의 코드에 의해 결정되며, 사용자는 프로그램을 실행시키는 것으로 이들 영역을 사용한다. 

고급 사용자나 관리자는 운영체제의 설정을 통해 이러한 값들을 변경할 수 있다. 예를 들어, Linux에서는 'ulimit' 명령을 사용하여 프로세스의 스택 크기를 조정할 수 있다.



# 0116 TIL
# WAS와 웹 서버의 차이점은 무엇인가요?

---

**웹 서버**는 정적 컨텐츠(HTML, CSS, JS, 이미지 등)를 제공하는 역할을 수행한다. 동적 컨텐츠 요청 시 요청을 WAS로 전달할 수도 있다. 대표적인 웹 서버로는 Apache, Nginx 등이 있다.

**WAS**는 서블릿 컨테이너 기능을 제공하고, 동적 컨텐츠를 생성하거나, 애플리케이션 로직을 실행하는 데 특화되어 있다. 대표적인 WAS로는 Tomcat이 있다. 

정리하자면, **웹 서버**는 **정적 컨텐츠 제공**에 특화되어 있고, **WAS**는 **동적 컨텐츠 생성과 데이터 처리에 특화**되어 있다. 

<aside>
💡

**WAS도 정적 컨텐츠를 제공할 수 있는데 웹 서버가 따로 필요한 이유는 무엇일까요?**

</aside>

WAS가 너무 많은 역할을 담당하면 과부하가 될 수 있다. 웹 서버를 따로 분리하면 WAS는 중요한 애플리케이션 로직에 집중할 수 있으며, 웹 서버는 정적 리소스를 처리하면서 업무 분담이 가능하다. 

또한, 시스템 리소스를 효율적으로 관리할 수 있다. 정적 컨텐츠가 많이 사용되는 경우에는 웹 서버를 증설하고, 애플리케이션 자원이 많이 사용되면 WAS를 증설하면 된다. 이 외에도 로드 밸런싱을 하거나, 캐싱 및 압축, HTTPS 등을 웹서버에서 처리하도록 할 수 있다. 

## 좀 더 자세히

### Web Server

웹 서버는 클라이언트(사용자)가 브라우저 주소창에 url을 입력하여 어떤 페이지를 요청하면, http 요청을 받아 들여 HTML 문서와 같은 정적 콘텐츠를 사용자에게 전달해주는 역할을 한다. 

**웹 서버의 임무는 대표적으로 2가지로**

1. 단순히 저장된 웹 리소스들을 클라이언트로 전달하고, 클라이언트로부터 콘텐츠를 전달받아 저장하거나 처리한다. 
2. 사용자로부터 동적인 요청이 들어왔을 때, 해당 요청을 웹 서버 자체적으로 처리하기 어렵기 때문에 WAS에게 요청한다.
    1. 대표적인 웹 서버 종류 : Apache, Nginx, IIS (window 전용 웹 서버)

### WAS (Web Application Server)

WAS 또한 웹 서버와 동일하게 HTTP 기반으로 동작한다. 웹 서버가 할 수 있는 기능 대부분이 WAS에서도 처리가 가능하며, 비즈니스 로직 (서버 사이드 코드) 을 처리할 수 있어 사용자에게 동적인 콘텐츠를 전달할 수 있다. 주로 데베 서버와 같이 수행된다. 

### 차이점

기능적으로 동일한 영역도 있고 WAS가 웹 서버 기능의 많은 부분을 포함하여 수행하지만 사용의 ‘목적’이 다르다.

웹 서버는 정적인 데이터를 처리하는 서버이다. 이미지나 단순 HTML 같은 정적 리소스들을 전달하며, WAS만을 이용할 때 보다 빠르고 안정적으로 기능을 수행한다. 

반면 WAS는 동적인 데이터를 위주로 처리하는 서버이다. DB와 연결되어 사용자와 데이터를 주고받고, 조작이 필요한 경우 WAS를 활용한다. 

### 추가

웹 서버 만으로도 분명 동적인 요청 처리가 가능하다. 예를 들면 PHP의 경우 WAS 없이 아파치나 Nginx 만을 통해서 동적인 요청 처리가 가능하다. 그걸 가능 하게 해주는 것이 **CGI** 인데, **웹 서버에 별도로 저장**해줘야 한다. 

CGI는 이름 그대로 인터페이스로서, 웹 서버상에서 프로그램을 동작시키기 위한 방법을 정의한 프로그램 (또는 스크립트) 이다. 

**CGI**

동적 컨텐츠를 제공하기 위해 웹 서버 내에 프로그래밍 기능이 들어가는 방식이다. 

즉, PHP, Perl, Python 등의 언어들을 CGI를 구현해놓았기 때문에 아파치에서 다양한 언어로 짜인 각 프로그램을 실행할 수 있다. 예를들어, 아파치에게 PHP 모듈을 설치했을 경우, 요청이 왔을 때 아파치는 HTTP 헤더를 분석하고 파싱하여 PHP로 파라미터를 넘겨준다. 그러면 PHP에서는 파라미터를 받아 응답할 HTML 문서를 만들어 아파치에게 전달한다. 

HTML 문서를 전달받은 아파치는 CSS, JS, IMG 등 정적인 자원들과 함께 브라우저로 반환해준다. 하지만 이 역시 CGI 효율이 떨어진다. CGI 만으로는 규모가 큰 웹 서비스를 구현하기는 사실상 어렵다. 

많은 프로그래머들이 JAVA를 견고한 언어라고 평가하는 이유도 여기에 있다. 자바 서블릿은 CGI를 사용하지 않는다. 그래서 WAS에 대해 설명할 때 대표적으로 자바, 톰캣, 아파치로 예시를 든다. 

### References

https://yozm.wishket.com/magazine/detail/1780/


# 1월 17일 TIL

## Record를 DTO로 사용하는 이유가 뭔가요?

Record는 Java 16에서 정식 출시된 특별한 유형의 클래스로 **불변성**을 기본으로 한다. 

기존의 클래스와 달리 모든 필드가 final 키워드로 선언되며, 객체 생성 후 변경할 수 없다. 또한 필드 선언만으로 자동으로 생성자, getter, equals(), hashCode(), toString() 등 메서드를 자동으로 생성해 주어 보일러 플레이트 코드를 줄일 수 있다. 이러한 특성으로 인해 멀티 스레드 환경에서 데이터가 의도치 않게 변경되지 않고 안전하게 전달할 수 있다. 

### DTO

```java
public class MemberDto {
	private final String name;
	private final String email;
	private final String gender;

	public MemberDto(String name, String email, String gender) {
		this.name = name;
		this.email = email;
		this.gender = gender;
	}

	public String getName() {
		return name;
	}
	
	public String getEamil() {
		return email;
	}
	
	public String getGender() {
		return gender;

}
```

### Record

```java
// Record. 생성자, getter, hashCode(), equals(), toString() 자동 완성
public record MemberDto(String name, String email, String gender) {}
```

### Record로 생성한 모든 객체는 DTO 인가요?

모든 Record 객체가 DTO인 것은 아니다. Record는 단순히 데이터를 캡슐화하는 역할을 하는데, DTO외에도 값 객체 (Value Objects) 등의 다양한 용도로 사용될 수 있다. 

```java
// 값 객체로 사용
public record Coordinates(double x, double y) {}
```

DTO는 계층 간 데이터 전송을 목적으로 하는 객체이고, VO는 도메인 모델 내에서 특정 값을 표현하는 객체로 사용된다. 따라서, Record는 이 두 가지 모두에 적합하게 사용할 수 있다. 

### Record와 VO를 비교해주세요.

Record와 VO는 모두 객체의 상태가 변경되지 않는 것을 보장한다. 또 데이터를 캡슐화하여 표현하는 데 초점을 맞춘다. 마지막으로 VO는 값 기반의 동등성을 가지며, Record도 동일한 필드 값을 가지면 동일한 객체로 간주된다는 점이 공통점이다. 

VO는 도메인 모델 내에서 특정 개념을 표현하고, 도메인 로직과 밀접하게 관련이 있다. 즉, VO는 비즈니스 로직이나 규칙을 가질 수 있다. 반면에 Record는 단순히 데이터를 캡슐화하여 저장하는데 의미가 있다. 

**결론적으로**, Record는 VO를 구현하는 데 적합하지만, VO의 모든 특성을 완벽히 대체하지는 않는다. VO는 더 넓은 도메인 맥락에서 사용되며, 비즈니스 로직을 포함할 수 있다. 

### Record의 한계는 무엇이 있을까?

Record는 extends를 사용하여 다른 클래스를 상속할 수 없고, 필드가 final로 선언되기 때문에 확장이 어렵다. 또 주로 데이터를 전달하려는 목적으로 설계되었기 때문에 비즈니스 로직을 포함하기에 적절하지 않다. 마지막으로 Java 14 또는 16 이전 버전에 호환이 불가능하다. 

### DTO와 Record 비교하기

**불변성**

Record는 기본적으로 불변성을 가지며, 한 번 생성된 Record 인스턴스의 데이터는 변경할 수 없다. 이러한 불변성 덕분에 데이터는 일관성을 유지하며, 추가적인 코드 없이도 스레드 안전(Threrad-Safe)하다. 

반면 DTO는 일반적으로 가변성을 가지며, 객체가 생성된 후에도 필드를 변경할 수 있다. DTO를 불변으로 만들려면 Setter 메서드를 사용하지 않거나, final 필드를 사용하여 신중하게 설계해야 한다. 

**보일러 플레이트 코드** 

Record의 큰 장점 중 하나는 보일러 플레이트 코드, 즉 반복적인 코드를 크게 줄일 수 있다는 점이다. DTO를 사용할 때는 보통 Getter, Setter, 생성자, equals(), hashCode(), toString() 메서드를 직접 작성해야 하지만, Record는 이러한 메서드를 자동으로 생성한다. 

반대로, DTO는 직접 코드를 작성해야 한다. 롬복 같은 도구를 사용하면 보일러 플레이트 코드를 줄일 수 있지만, Record만큼 간단하지 않다. 

**데이터 표현 방식**

Record는 데이터를 간결하고 직관적으로 표현하는 방법을 제공한다. Record 선언에는 필드만 포함되므로 코드가 더 깔끔하고 읽기 쉽다. 특히 데이터 모델이 많은 프로젝트에서 유지보수가 용이하다. 

**커스터마이징**

DTO의 장점 중 하나는 커스터마이징이 용이하다는 점이다. DTO에서는 데이터 유효성 검사, 데이터 변환 메서드 또는 비즈니스 로직을 추가할 수 있다. 

반면, Record는 커스터마이징이 제한적이다. Record는 가볍고 불변성을 유지하도록 설계되었기 때문에 내부 상태를 수정하거나 복잡한 로직을 쉽게 추가할 수 없다. 만약 데이터 객체에 커스터마이징된 동작 로직이 필요하다면 DTO가 더 유연한 선택이다. 

**함수형 프로그래밍과의 연관성**

함수형 프로그래밍의 핵심 원칙 중 하나는 불변성(Immutability)입니다. 즉, 데이터 객체가 한 번 생성된 후에는 변경되지 않아야 한다는 것이다. Record는 기본적으로 불변이므로 함수형 프로그래밍 원칙과 잘 맞는다. 따라서 불변 데이터 객체를 사용하고자 하는 시스템에 적합한 선택이다. 

반면 DTO는 가변성을 가지며, 불변으로 만들려면 수동적인 설정이 필요하다. DTO는 상태 변경이 흔한 객체 지향 프로그래밍 스타일에 더 적합하다. 

### DTO와 Record는 언제 사용해야 할까?

**DTO를 사용해야 할 때** 

1. **데이터 수정이 필요한 경우**
    
    객체의 데이터를 생성 후 수정해야 할 때는 DTO가 더 적합하다. DTO는 보통 가변적이어서 필드 값을 필요에 따라 변경할 수 있다.  객체의 수명 주기 동안 데이터가 계속해서 업데이트 되는 상황에서 유용하다. 
    
2. **추가적인 동작이나 검증 로직이 필요한 경우**
    
    DTO는 검증, 변환, 또는 추가 메서드 등 맞춤형 동작을 추가하는데 더 유연하다. 데이터 객체가 단순히 데이터를 전달하는 것 이상으로 동작해야 할 때 적합하다. 
    
3. **이전 버전의 Java(16 이전 버전)와 호환이 필요한 경우**
    
    Java 16 이전 버전을 사용하는 프로젝트라면 Record를 사용할 수 없다.
    

**Record를 사용해야 할 때**

1. **간결하고 불변성을 가진 데이터 전달 객체가 필요한 경우**
    
    Record는 가볍고 불변성을 가진 객체로 데이터를 전달해야 할 때 이상적이다. Record는필수적인 메서드들을 자동으로 생성해 주기 때문에 데이터 표현을 간결하고 효율적으로 처리할 수 있다. 
    
    예시 ) MSA에서 서비스 간 데이터를 전달할 때 데이터를 수정할 필요가 없다면 Record가 완벽한 선택일 수 있다. 
    
2. **읽기 전용 데이터 전송이 필요한 경우**
    
    애플리케이션에서 데이터를 전달하기만 하고 수정할 필요가 없다면 Record를 사용하는 것이 좋다. Record는 불변성을 보장하여 데이터 일관성을 유지하기 때문에, 데이터베이스에서 서비스 계층으로 또는 서비스 간 데이터를 전달하는데 적합하다. 
    
3. **최신 자바 애플리케이션에서** 
    
    Java 16 이상을 사용하고 있다면 Record를 충분히 활용할 수 있다. Record는 최신 Java 애플리케이션에서 데이터 표현을 간소화하도록 설계되었으며, 기존 DTO가 가지고 있던 불필요한 반복적인 코드를 줄이는 데 도움이 된다. 
    

### 성능 고려 사항은?

DTO와 Record의 성능을 비교할 때 차이는 크지 않지만, 몇 가지 중요한 요소들을 고려해야 한다. 

**메모리 효율성**

Record는 설계상 간결하므로 DTO보다 메모리를 조금 덜 사용할 수 있다. 그 이유는 Record가 직접 Getter, Setter, equals(), hashCode(), toString() 같은 메서드를 구현할 필요가 없기 때문이다. 이러한 메서드들이 Java 컴파일러에 의해 자동으로 최적화 되어 생성되므로 , 메모리 사용량이 줄어든다. 

**불변성과 스레드 안정성**

Record는 불변이므로 특히 멀티 스레드 환경에서 성능상의 이점을 제공한다. Record는 불변이기 때문에 스레드 간에 공유될 때 동기화나 잠금(locking) 메커니즘이 필요하지 않다. 이는 스레드 간 경쟁으로 성능이 저하되는 상황에서 성능을 향상할 수 있다. 

반면, 가변 DTO를 멀티 스레드 환경에서 사용할 경우, 스레드 안정성을 보장하기 위해 접근을 동기화하거나 다른 메커니즘을 사용해야 하므로 추가적인 부담이 생기고 애플리케이션이 느려질 수 있다. 

**가비지 컬렉션**

DTO와 Record 모두 일반적인 Java 객체이므로 동일한 가비지 컬렉션 처리에 따라 관리된다. 하지만 Record가 더 간결하므로 메모리에 적은 객체가 생성되거나 유지될 수 있어, 가비지 컬렉션이 조금 더 빠르게 이루어질 가능성이 있다. 이는 대량의 데이터 객체를 처리하는 장기 실행 애플리케이션에서 성능 향상에 기여할 수 있다. 

**CPU  오버헤드**

Record는 컴파일러에 의해 자동 생성되며 성능을 최적화하도록 설계되어 있어, 객체 생성, 메서드 호출, 비교 작업에서 CPU 성능이 조금 더 향상될 수 있다. 특히 복잡한 DTO의 경우, 수동으로 구현된 메서드에서 비 효율성이 발생할 수 있는데, Record는 일관되고 최적화된 방식으로 이러한 작업을 처리하므로 효율적이다. 

**실제 성능**

실제로는 DTO와 Record 간의 성능 차이는 대부분의 애플리케이션에서 매우 적거나 무시할 만한 수준일 것이다. Record의 간결함이 특정 시나리오에서 약간의 성능 향상을 가져올 수 있지만, 실제로는 대용량 데이터 처리, 높은 처리량을 요구하는 애플리케이션, 또는 리소스가 제한된 환경(예시 : 모바일 또는 IoT 장치)에서만 눈에 띌 정도의 성능 차이를 경험할 수 있다.


# 1월 20일 TIL

## OS

> 💡**단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.**


[[운영체제] 프로세스 스케줄러(단기,중기,장기)](https://kosaf04pyh.tistory.com/191)

**단기 스케쥴러 : 어떤 프로세스에게 CPU를 할당해 줄 것인가 ?**

> CPU 하나의 작업만을 처리한다. 따라서 Ready Queue에서 CPU를 할당할 하나의 프로세스를 선택해야 한다.
> 

CPU 스케쥴러라고도 하며, 준비 상태의 프로세스 중에서 어떤 프로세스를 다음 번에 실행 상태로 만들 것인지 결정한다. 시분할 시스템에서 타이머 인터럽트가 발생하면 단기 스케쥴러가 호출된다. 

일반적으로 스케쥴러라 함은 단기 스케쥴러를 의미하며, 단기 스케쥴러는 미리 정한 스케쥴링 알고리즘에 따라 CPU를 할당 할 프로세스를 선택한다.

단기 스케쥴러는 밀리 세컨드 (ms) 이하의 시간 단위로 매우 빈번하게 호출되기 때문에 수행 속도가 충분히 빨라야 한다.

- **시분할 시스템**
    
    윈도우 운영체제가 제공하는 주요 기능 중 하나이다. 여러 프로세스가 사용하는 시스템에서 컴퓨터가 자원을 시간적으로 분할해주어서 사용자들의 프로그램을 번갈아가며 처리해줌으로써 각 프로세스에게 독립된 컴퓨터를 사용하는 느낌을 주는 것이다. 
    

**중기 스케쥴러 : 메모리에 적재된 프로세스 수 관리**

> Swapper 라고도 불린다. 메모리에 적재된 프로세스 수를 관리하는 스케쥴러이다. **스와핑(Swapping)** : 일부 프로세스를 메모리에서 디스크로 보내고(**swap-out**), 시간이 흘러 메모리에 여유가 생기면 다시 적재(**swap-in**)한다
> 

너무 많은 프로세스에게 메모리를 할당해 시스템의 성능이 저하되는 경우 이를 해결하기 위해 메모리에 적재된 프로세스의 수를 동적으로 조절하기 위해 추가된 스케쥴러이다. 

만약 메모리에 많은 수의 프로세스가 적재되어 프로세스 당 보유하고 있는 메모리량이 극도로 적어지게 되면 CPU 수행에 당장 필요한 프로세스의 주소 공간 조차도 메모리에 올려놓기 어려운 상황이 발생하게 된다. 이런 경우 메모리에 올라와 있는 프로세스 중 일부로 부터 메모리를 통째로 빼앗아 그 내용을 디스크의 스왑영역에 저장해 둔다. 이것을 스왑 아웃 이라고 한다.

디스크로 스왑 아웃 시켜야 하는 경우 봉쇄 상태에 있는 프로세스들을 첫번째로 스왑 아웃 시킨다. 이유는 봉쇄 상태의 프로세스들은 당장 CPU를 획득할 가능성이 없기 때문이다. 

봉쇄 상태의 프로세스들을 스왑 아웃 시켜도 문제가 해결되지 않는 경우 중기 스케쥴러는 타이머 인터럽트가 발생해 준비 큐로 이동하는 프로세스를 추가적으로 스왑 아웃 시킨다. 중기 스케쥴러는 이러한 방식으로 장기 스케쥴러와 마찬가지로 메모리에 올라와 있는 프로세스의 수를 조절하는 역할을 한다. 

중기 스케쥴러의 등장으로 프로세스의 상태에는 중지 상태가 추가 되었으며, 중지 상태의 프로세스는 메모리를 통째로 빼앗기고 디스크로 스왑 아웃 된다. 중지 상태는 중지 준비 상태와 봉쇄 중지 상태가 있다. 

- 중지 준비 : 준비 상태의 프로세스가 중기 스케쥴러에 의해 디스크로 swap out
- 봉쇄 중지 : 봉쇄 상태의 프로세스가 중기 스케쥴러에 의해 디스크로 swap out

중지 봉쇄 상태에던 프로세스가 봉쇄 되었던 조건을 만족하게 되면 이 프로세스의 상태는 중지 준비 상태로 바뀌게 된다. 중지 상태에 있는 프로세스들은 중지 준비 상태이든 중지 봉쇄 상태이든 관계 없이 메모리를 조금도 보유하지 않고 디스크에 통째로 스왑 아웃된 상태로 존재한다. 

**장기 스케쥴러 : 어떤 프로세스를 준비 큐에 넣을 것인가?**

> 메모리는 한정되어 있기 때문에, 실행할 수 있는 프로세스 보다 많은 프로세스가 메모리에 올라오면 대용량 메모리 (일반적으로 하드 디스크)에 임시로 저장된다. 장기 스케쥴러는 하드 디스크의 프로세스 중 하나를 선택하여 메모리를 할당하고, Ready Queue로 보내는 역할을 한다.
> 

작업 스케쥴러 라고도 부르며 어떤 프로세스를 준비 큐에 삽입할지를 결정하는 역할을 한다.

디스크에서 하나의 프로그램을 가져와 커널에 등록하면 프로세스가 되는데, 이 때 디스크에서 어떤 프로그램을 가져와 커널에 등록할지 (준비 큐에 등록할지) 결정한다.

장기 스케쥴러는 수십 초 내지 수 분 단위로 가끔 호출되기 때문에 상대적으로 속도가 느린 것이 허용된다. 또한 장기 스케쥴러는 메모리에 동시에 올라가 있는 프로세스의 수를 조절하는 역할을 한다. 

> ❗**현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?**

현대의 시분할 시스템에서 사용되는 운영체제 에서는 일반적으로 장기 스케쥴러를 두지 않는 경우가 대부분이다. 과거에는 적은 양의 메모리를 많은 프로세스들에게 할당하면 프로세스 당 메모리 보유량이 적어져 장기 스케쥴러가 이를 조절하는 역할을 했지만, 현대의 운영체제에서는 프로세스가 시작되면 장기 스케쥴러 없이 바로 그 프로세스에 메모리를 할당해 준비 큐에 넣어주게 된다. 


> ❗**프로세스의 스케쥴링 상태에 대해 설명해 주세요.**


[스케쥴러에 대해 설명해주세요](https://velog.io/@zsmalla/%EC%8A%A4%EC%BC%80%EC%A5%B4%EB%9F%AC%EC%97%90-%EB%8C%80%ED%95%B4-%EC%84%A4%EB%AA%85%ED%95%B4%EC%A3%BC%EC%84%B8%EC%9A%94)

**프로세스의 상태와 주의할 것.**

- 프로세스의 상태는 프로세스가 실행되는 동안 어떤 단계에 있는지를 나타내는 것이다. OS는 여러 상태를 통해 프로세스의 생애 주기를 관리한다.
- 참고
    - **New (새로운 상태)**: 프로세스가 생성되는 단계로, 아직 실행되지 않은 상태입니다.
    - **Ready (준비 상태)**: CPU가 할당되기를 기다리고 있는 상태입니다. 즉, 실행될 준비가 되어 있지만, CPU 자원을 기다리고 있는 상태입니다.
    - **Running (실행 상태)**: CPU에서 실행 중인 상태입니다.
    - **Blocked (블록 상태) 또는 Waiting (대기 상태)**: 프로세스가 특정 이벤트나 자원을 기다리고 있어 실행될 수 없는 상태입니다.
    - **Terminated (종료 상태)**: 프로세스가 실행을 마치고 종료된 상태입니다.

**스케쥴링의 상태**

OS에서 프로세스를 스케쥴링 하는 과정에서 각 프로세스가 어떤 상태에 있는지를 나타내는 개념

![image](/uploads/89c7a72036c54d72e54cf7398a5624a8/image.png){width=651 height=344}

![image](/uploads/4ae6ebdd2ce472b640f7b0c3679f7c81/image.png){width=654 height=222}

프로세스의 상태는 메모리 할당 상태에 따라 크게 **Active(Swapped-in)** 상태와 **Suspended(Swapped-out)** 상태로 나뉠 수 있다. 

**Ready** : 메모리를 할당 받은 프로세스는 레디 큐에서 CPU 할당을 대기한다.

**Running** : CPU 스케쥴링에 의해 CPU를 할당 받은 프로세스는 Running 상태가 되어 작업을 수행한다. 작업 수행 중 스케쥴링에 의해 CPU를 선점 당해 Ready 상태가 되거나 I/O 인터럽트 등으로 Block 되어 Asleep 상태로 상태가 전이될 수 있다.

**Asleep** : 인터럽트 등으로 프로세서 외의 다른 자원을 기다리는 상태이다. 해당 자원을 할당받으면 Running 상태가 아닌 다시 Ready 상태가 되어 CPU 할당을 대기한다. 

**Suspended Ready, Suspended Block** : 메모리를 할당 받지 못하거나 빼앗긴 상태이다. 빼앗기게 된 경우 Memory image를 Swap device에 보관(swap-out) 했다가 자원을 다시 할당받을 때 해당 memory image를 통해 상태를 복구 (swap-in)하게 된다.   


> ❗**preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?**

[스케쥴러에 대해 설명해주세요](https://velog.io/@zsmalla/%EC%8A%A4%EC%BC%80%EC%A5%B4%EB%9F%AC%EC%97%90-%EB%8C%80%ED%95%B4-%EC%84%A4%EB%AA%85%ED%95%B4%EC%A3%BC%EC%84%B8%EC%9A%94)

Asleep 상태는 작업 실행이 불가능한 상태로, 자원 할당을 대기 하는 등 사전 정의 된 다른 작업이 완료될 때 까지 작업 수행이 중지되는 상태이다. 따라서 Asleep 상태는 premitive 및 non-premitive 스케쥴링 모두에서 존재할 수 없다. 

**선점형 스케쥴링**에서, 현재 실행중인 프로세스가 CPU를 빼앗길 수 있다. 그러나 Asleep 상태에 있는 프로세스는 이미 CPU를 사용하지 않는다. 따라서 프로세스가 이 상태의 프로세스를 선택하지 않는다.

**비선점형 스케쥴링**에서, 현재 실행 중인 프로세스가 자발적으로 CPU를 반납해야 다른 프로세스가 실행된다. 마찬가지로 Asleep 상태에 있는 프로세스는 CPU를 사용하지 않으므로, 이 상태의 프로세스도 스케쥴러가 선택할 수 없다.

**결론**

- **Asleep (Blocked)** 상태에 있는 프로세스는 **실행을 기다리는 상태**라서, **CPU를 사용하지 않기 때문에** 스케줄러가 이 프로세스를 실행할 수 없다.
- 그래서 **Preemptive**와 **Non-preemptive** 스케줄링 방식에서는 **Asleep 상태의 프로세스를 실행하려고 하지 않는다**.

즉, **Asleep** 상태에 있는 프로세스는 **실행될 수 없기 때문에**, 이 상태는 **스케줄링에서 선택되지 않는다**.


> ❗**Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?**


Active(Swapped-in) 상태에서의 process는 더 이상 작업을 수행할 수 없게 되기 때문에 Swap-out 되어 suspended(Swapped-out) 상태로 전이된다.

이 때 메모리 부족으로 필요한 데이터를 가져올 수 없을 경우 block 되어 다른 프로세스가 끝나기를 기다리거나, OS가 프로세스를 강제로 종료시켜 Terminated 상태로 전이된다. 


> 💡**컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?**


**context ?**

CPU가 해당 프로세스를 실행하기 위해 필요한 해당 프로세스의 정보를 뜻한다. CPU는 context를 가지고 프로세스들을 실행 시킨다. 

1. **컨텍스트 저장**
    1. 현재 프로세스의 상태를 저장한다.
    2. CPU 레지스터 값, 프로그램 카운터, 스택 포인터 등 프로세스의 실행 상태를 나타내는 관련 데이터 값을 의미한다. 
2. **실행될 다음 컨텍스트 불러오기**
    1. OS는 실행될 다음 프로세스의 저장된 컨텍스를 가져온다. 
3. **PCB 업데이트**
    1. 프로세스와 관련된 데이터 구조인 프로세스 제어 블록 (PCB)을 프로세스 상태와 실행 기록의 변경 사항을 반영하도록 업데이트 한다. 
4. **실행 전환**
    1. CPU는 새로운 프로세스의 컨텍스트로 실행을 전환한다. 



> ❗**프로세스와 스레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?**



다른 프로세스끼리의 스위칭인 프로세스 컨텍스트 스위칭은 가상 메모리 주소 관련 처리를 추가로 수행한다. 

- 같은 프로세스에 속한 스레드들끼리는 공유하는 메모리 지역이 있는데 (코드영역, 데이터 영역, 힙 영역) 따라서 스레드 들끼리의 컨텍스트 스위칭은 같은 프로세스에 속하기 때문에 스위칭이 일어나도 메모리와 관련해서는 챙겨줘야 할 부분이 없다.
- 하지만, 프로세스 간의 컨텍스트 스위칭은 메모리 주소 체계가 다르기 때문에 이 때는 메모리 주소 관련된 처리를 추가로 수행해야 한다.
- 그렇기 때문에 MMU(Memory Management Unit) 와 TLB (Translation Lookaside Buffer)도 관리를 해줘야 한다.
    - MMU : 가상 메모리와 물리 메모리 사이의 주소 변환을 담당한다. 프로세스가 물리 메모리에서 할당되는 위치를 추상화하여 각 프로세스가 독립적인 주소 공간을 가지고 있는 것 처럼 만든다.
    - TLB : MMU 내부에 존재하는 캐시 메모리. 가상 주소를 물리 주소로 변환하는 과정이 상대적으로 느릴 수 있기 때문에, 이러한 변환의 결과를 TLB에 저장해두고 빠르게 사용하는 데에 목적이 있다.

스레드 간의 스위칭에서는 실행되고 있던 스레드의 상태를 저장하고, 새로 실행될 스레드의 상태를 로딩하는 것으로 해결되지만 프로세스 간의 스위칭에서는 위의 작업에 더불어서 MMU가 실행 될 작업의 메모리를 보도록 해야 하고 캐시 역할을 하는 TLB는 완전히 비워줘야 한다.

## CKA
# Recap - ReplicaSets

---

**쿠버네티스 컨트롤러에 관해 알아보자.** 

쿠버네티스 객체를 모니터링하고 그에 따라 반응하는 프로세스이다. 여기서는 복제 컨트롤러에 대해 이야기 해보자.

## 복제본은 무엇이고 복제 컨트롤러는 왜 필요할까?

**첫 번째 시나리오로 돌아가서, 응용 프로그램을 실행하는 단일 포드가 있다고 하자. 어떤 이유로든 앱이 다운되고 캡슐이 고장나면 어떻게 될까?**

사용자는 더 이상 우리 앱에 접속할 수 없게 된다. 사용자가 앱에 대한 액세스를 잃지 않도록 하려면 한 개 이상의 인스턴스나 pod가 동시에 실행되어야 한다. 그래야 하나가 실패해도 다른 하나에서는 우리 응용 프로그램이 실행된다. 

복제 컨트롤러는 쿠버네티스 클러스터에 있는 단일 포드의 다중 인스턴스를 실행하도록 도와준다. 즉, 고가용성을 제공한다. 

**Pod 하나를 가질 예정이라면 Replication controller을 사용할 수 없다는 걸까?** 

그것은 아니다. 

포드가 하나 뿐이어도 복제 컨트롤러는 기존의 포드가 고장났을 때 자동으로 새로운 포드를 불러올 수 있다. 따라서 복제 컨트롤러는 포드가 항상 실행되도록 보장한다. 1개든, 100개든.

복제컨트롤러가 필요한 또 다른 이유는 **여러 개의 포드를 만들어 로드를 공유하기 위해서이다.** 

![image](/uploads/9b71bd45d2913c91a4de80d0a5a21ccd/image.png){width=621 height=317}

단일 포드가 사용자 집합을 담당한다고 할 떄, 사용자 수가 증가하면 포드를 하나 더 설치해서 두 부분의 균형을 잡는다. 수요가 더 증가하고 첫 번째 노드에 리소스가 바닥난다면 클러스터 내 다른 노드에 추가 부품을 배포할 수 있다. 복제 컨트롤러는 클러스터 내 여러 노드로 뻗어있다. 서로 다른 노드의 여러 포드에 걸쳐 부하를 분산하는데 도움이 되고 수요가 증가하면 앱 스케일도 조정할 수 있다. 

## 복제 컨트롤러와 복제본 세트

용도는 같지만 같지는 않다. 

복제 컨트롤러는 구식 기술로 복제본 세트로 대체되고 있다. 

**복제본 집합**은 복제를 설정하는 새로운 권장 방법이다. 

### 복제본 컨트롤러를 어떻게 생성할까?

복제 컨트롤러 정의 파일을 만드는 것부터 시작할건데, pod 만들 때 했던 것 처럼 4개의 섹션으로 나누어 작성한다. API 버전, 종류, 메타데이터, 그리고 스펙으로 나눈다. 

복제 컨트롤러의 API 버전의 경우 V1에서 지원한다. 

![image](/uploads/92acfae02be2d270822e62d095547f25/image.png){width=673 height=360}

쿠버네티스 정의 파일에서 **spec은 우리가 만드는 개체 안에 무엇이 있는지 정의**한다. 

**복제 컨트롤러는 포드에 여러 개의 인스턴스를 만든다. 그런데 어떤 포드일까?**

스펙 아래에 템플릿 섹션을 생성한다. 이는 복제 컨트롤러가 복제본을 만들기 위해 사용할 포드 템플릿을 제공하기 위해서이다. 지난시간에 Pod정의한 것을 template 아래에 넣어주면 된다. 단, api 버전과 kind는 뺴주면 된다. 

꼭 template 섹션 아래 있어야 하고, indent 신경쓰자

파일을 보면 두 개의 메타 데이터 섹션이 있게 된다. 하나는 복제 컨트롤러 용이고, 다른 하나는 포드 용이다. 또한 각각 두 개의 스펙 섹션이 있다. 두 개의 definition 파일을 함께 중첩 시킨 것이다. 복제 컨트롤러는 부모이고, 포드 정의는 자식이다. 

이제 복제 컨트롤러에 복제본이 얼마나 필요한지만 적어주면 된다. (replicas 를 정의해주면 된다)

replica는 template과 같은 인덴트를 가지면 되고, spec의 자식이어야 한다. 

### 복제 세트는 어떻게 생성할까 ?

늘 그렇듯 API 버전, kind, metadata, spec을 생성한다. API 버전은 apps/v1 이다. 

![image](/uploads/c441e56a835541209d20ac75a14077ec/image.png){width=659 height=350}

버전에 유의하자. 버전에 유의하지 않으면 다음과 같은 에러가 뜬다. 

거의 비슷하게 작성된다.

하지만 하나의 큰 차이점이 있다. 복제본 세트에는 selector section의 정의가 필요하다. selector는 복제품 세트로 그 밑에 놓인 pod를 식별할 수 있게 해준다. 

**왜 템플릿에서 포드 정의 파일 내용을 제공했는데 그 아래 어떤 포드가 있는지 정의해야 할까?**

replica set은 replica set의 일부로 만들어지지 않은 포드도 관리할 수 있다. 

만약, selector에 지정된 레이블과 일치하는 복제본 세트를 만들기 전에 pod가 만들어졌다고 치자. 복제본을 만들 때 복제본 세트는 그 포드를 고려하게 된다. 

> **정리**
> 

ReplicaSet은 일정 수의 pod를 유지하기 위해 사용된다. 이를 위해 selector을 사용해서 특정 레이블을 가진 pod를 식별한다. selector는 “**이 레이블을 가진 pod들을 관리하겠다**” 라는 기준을 설정하는 역할을 한다. 

만약, replicaSet이 생성되기 전에 이미 selector 조건에 맞는 pod가 존재한다면, ReplicaSet은 새로운 Pod를 만들지 않고 기존 pod를 관리대상으로 삼는다. 

따라서, 관리해야 할 replica의 수를 조정할 때, 이미 존재하는 pod를 포함한 상태에서 추가적으로 필요한 Pod만 생성한다. 

예시로,

1. 어떤 ReplicaSet의 Selector가 `{app: myapp}`이라고 하자. 
2. 이 ReplicaSet을 생성하기 전에 `{app: myapp}` 레이블을 가진 파드가 이미 2개 있었다고 가정하자.
3. ReplicaSet이 "복제본 수를 3개로 유지하라"고 설정되어 있다면:
    - 새 파드를 3개 전부 만들지 않고, 기존 2개의 파드를 포함하여 1개만 추가로 생성한다.

이렇게 기존 파드와 새로 생성할 파드를 조화롭게 관리하는 것이 ReplicaSet의 역할이다.

복제 컨트롤러의 경우 selector가 필수 필드는 아니지만 여전히 사용 가능하다. 

Replica Set은 matchLabels에 작성해주어야 한다. 지정된 라벨과 pod의 라벨을 매치시켜주어야 한다. 

Replica Set은 이미 생성된 기존의 pod를 모니터 하는 데 사용할 수 있다. 만약 실제로 만들지 않았다면 Replica set가 만들어주는 것이다. Replica Set의 역할은 Pod를 모니터하고 하나가 고장나면 새 Pod를 배포하는 것이다.

**Replica Set은 어떤 pod를 모니터 할 지 어떻게 알까?** 

라벨을 통해서 알 수 있다. 같은 라벨을 사용하는 것들을 관리한다. 

ReplicaSet이 새 포드를 생성하려면 템플릿 정의 섹션이 필요하다. 따라서 복제본 수를 3개를 유지하라고 하고 이미 3개가 있는 상황에서도 템플릿 정의 섹션은 꼭 제공되어야 한다. 

**ReplicaSet의 개수를 늘리는 방법이 있을까 ?**

1. 정의 파일의 replicas를 업데이트 한 후, kubectl 명령어를 통해 같은 파일을 명시한다.
    
    ```
    kubectl replace -f replicaset-definition.yml
    ```
    
2. kubectl scale 명령어를 사용한다
    
    ```
    kubectl scale --replicas=6 -f replicaset-definition.yml
    ```
    
    ```
    kubectl scale --replicas=6 replicaset myapp-replicaset
    ```
    
![image](/uploads/44f725b50a7d63d33f10bf4662552047/image.png){width=615 height=110}
    

> **뭐라는거냐면**
> 

replicaset 자체는 6개로 변해서 kubectl get replicaset하면 6개로 변경된게 나오는데, 정의 파일 자체는 vi 편집해서 보면 정의파일 자체가 변경된 것은 아니라는 말이다. 

### Commands

```
kubectl create -f replicaset-definition.yml
```

```
kubectl get replicaset
```

```
kubectl delete replicaset myapp-replicaset // also deletes all underlying PODS
```

```
kubectl replace -f replicaset-definition.yml
```

# Deployments


### 롤링 업데이트

여러개의 인스턴트를 업그레이드 할 때, 사용자가 앱에 접속하는데 영향을 줄 수 있으니 하나씩 업그레이드 하는 것이다. 

**만약, 우리가 한 업그레이드 중에 뜻밖의 오류가 생겨서 최근의 변화를 취소해야 한다면 ?**

최근에 행해진 변화를 되돌릴 수 있어야 한다.

환경에 여러가지 변화를 주어야 한다. 예를 들어, 웹 서버 버전을 업그레이드 하거나 환경을 조정할 수 있다. 리소스 할당도 수정할 수 있다. 

명령이 실행되지마자 각 변경을 적용해선 안된다. 우리의 환경에 일시 중지를 적용해 변화를 준 다음 다시 시작해야 한다.

쿠버네티스 배포와 함께 위 모든 기능이 함께 사용 가능하다. 

![image](/uploads/1764a5a18a6e2f3943ae2b57dc86223d/image.png){width=663 height=276}

계층 구조에서 배포가 더 높다. 배포를 통해 우리는 하부 인스턴스를 매끄럽게 업그레이드 할 수 있다. 

업데이트, 변경 취소, 일시 정지, 필요에 따라 변경 사항을 재개한다. 

### 우리는 배포를 어떻게 만들까?

배포 정의 파일의 내용은 ReplicaSet 정의 파일과 아주 유사하다. 배포가 되는 파일을 제외하면.

![image](/uploads/33eb7a7351ddbbc5a4402c15928f1daf/image.png){width=657 height=341}

kind 제외하면 똑같음. Deployment 여야함. 대소 구분하기

deployment가 자동으로 replicaSet을 생성하고, replicaSet이 pods를 생성한다. 

### 생성된 객체 전체 보기
```
kubectl get all
```


# 명령어 모음집


### Pod 생성

```
kubectl run nginx --image=nginx
```

### Pod YAML 생성 (생성하지 않음)

실제로 pod를 생성하지 않고 yaml 형식으로 그 내용을 출력할 수 있다. YAML을 미리 확인하거나 수정하려는 경우 유용하다. 

```
kubectl run nginx --image=nginx --dry-run=client -o yaml
```

### Deployment 생성

```
kubectl create deployment --image=nginx nginx
```

### Deployment YAML 생성 (생성하지 않음)

실제 deployment를 생성하지 않고 YAML 파일을 생성할 수 있다. 

```
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
```

### YAML 파일로 저장

```
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml
```

### YAML 파일을 사용하여 Deployment 생성

```
kubectl create -f nginx-deployment.yam
```

### Replica 수 지정

```
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml
```

### Help 보기
```
kubectl create deployment --help
```

# 1월 21일 TIL

## OS

> 💡**컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?**

**context ?**

CPU가 해당 프로세스를 실행하기 위해 필요한 해당 프로세스의 정보를 뜻한다. CPU는 context를 가지고 프로세스들을 실행 시킨다. 

1. **컨텍스트 저장**
    1. 현재 프로세스의 상태를 저장한다.
    2. CPU 레지스터 값, 프로그램 카운터, 스택 포인터 등 프로세스의 실행 상태를 나타내는 관련 데이터 값을 의미한다. 
2. **실행될 다음 컨텍스트 불러오기**
    1. OS는 실행될 다음 프로세스의 저장된 컨텍스를 가져온다. 
3. **PCB 업데이트**
    1. 프로세스와 관련된 데이터 구조인 프로세스 제어 블록 (PCB)을 프로세스 상태와 실행 기록의 변경 사항을 반영하도록 업데이트 한다. 
4. **실행 전환**
    1. CPU는 새로운 프로세스의 컨텍스트로 실행을 전환한다. 


> ❗ **프로세스와 스레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?**


다른 프로세스끼리의 스위칭인 프로세스 컨텍스트 스위칭은 가상 메모리 주소 관련 처리를 추가로 수행한다. 

- 같은 프로세스에 속한 스레드들끼리는 공유하는 메모리 지역이 있는데 (코드영역, 데이터 영역, 힙 영역) 따라서 스레드 들끼리의 컨텍스트 스위칭은 같은 프로세스에 속하기 때문에 스위칭이 일어나도 메모리와 관련해서는 챙겨줘야 할 부분이 없다.
- 하지만, 프로세스 간의 컨텍스트 스위칭은 메모리 주소 체계가 다르기 때문에 이 때는 메모리 주소 관련된 처리를 추가로 수행해야 한다.
- 그렇기 때문에 MMU(Memory Management Unit) 와 TLB (Translation Lookaside Buffer)도 관리를 해줘야 한다.
    - MMU : 가상 메모리와 물리 메모리 사이의 주소 변환을 담당한다. 프로세스가 물리 메모리에서 할당되는 위치를 추상화하여 각 프로세스가 독립적인 주소 공간을 가지고 있는 것 처럼 만든다.
    - TLB : MMU 내부에 존재하는 캐시 메모리. 가상 주소를 물리 주소로 변환하는 과정이 상대적으로 느릴 수 있기 때문에, 이러한 변환의 결과를 TLB에 저장해두고 빠르게 사용하는 데에 목적이 있다.

스레드 간의 스위칭에서는 실행되고 있던 스레드의 상태를 저장하고, 새로 실행될 스레드의 상태를 로딩하는 것으로 해결되지만 프로세스 간의 스위칭에서는 위의 작업에 더불어서 MMU가 실행 될 작업의 메모리를 보도록 해야 하고 캐시 역할을 하는 TLB는 완전히 비워줘야 한다. 


> ❗**컨텍스트 스위칭이 발생할 때, 기존의 프로세스 정보는 커널스택에 어떠한 형식으로 저장되나요?**


PCB에 PC, Register, 메모리 관리 정보, 스택 및 힙 포인터, 기타 프로세스 정보 등의 정보를 포함하여 저장한다. 


> ❗**컨텍스트 스위칭은 언제 일어날까요?**

- 주어진 Time Slice(Time Quantum)를 다 사용함
- I/O 작업을 해야 함
- 다른 리소스를 기다려야 함
- 인터럽트(Interrupt)


> 💡**프로세스 스케줄링 알고리즘에는 어떤 것들이 있나요?**


**FCFS (First Come First Served)**

- 특징
    - **비선점형 스케줄링**
        - 이미 할당된 CPU를 다른 프로세스가 강제로 빼앗아 사용할 수 **없는** 스케줄링 기법.
        - 짧은 작업이 긴 작업을 기다리는 경우가 발생할 수 있다.
        - 프로세스 응답 시간의 예측이 용이하며, 일괄 처리 방식에 적합하다.
    - 먼저 도착한 프로세스가 먼저 CPU를 선점한다.
- 단점
    - **Convoy Effect** : 실행시간이 짧은 프로세스들이 실행시간이 긴 프로세스를 계속해서 기다리면서 효율성이 저하됨

**SJF (Shortest Job First)**

- 특징
    - 실행시간이 가장 짧은 프로세스부터 실행함
    - 비선점 스케줄링, 이미 긴 프로세스가 실행중이라면 새로 도착한 짧은 프로세스는 기다리기 해야한다.
- 단점
    - **Starvation** : 실행시간이 긴 프로세스가 영원히 CPU를 할당 받을 수 없게 됨.

**SRTF (Shortest Remaining Time First), SRT**

- 특징
    - 선점형 스케쥴링. 현재 실행 중인 프로세스보다 우선순위가 더 높은 프로세스가 도착하면 CPU를 뺏긴다.
    - 새로운 프로세스가 도착할 때 마다 새로운 스케쥴링을 진행한다.
    - 새로운 프로세스가 들어온 시점에서 현재까지 남은 실행시간이 가장 적은 프로세스를 먼저 실행한다. ⇒ 새로 들어온 가장 짧은 프로세스가 오자마자 실행될 수 있다.
- 단점
    - Starvation
    - 새로운 프로세스가 올 때 마다 스케줄링을 다시 하므로, 프로세스의 정확한 CPU Burst Time을 측정할 수 없다.

**Priority Scheduling**

- 특징
    - 우선순위가 높은 프로세스가 CPU를 선점하도록 하는 스케줄링
        - 우선순위는 숫자가 작을수록 높은 것이다.
    - 선점, 비선점 스케줄링 방식 모두 사용 가능하다.
        - **선점형** : 더 높은 우선순위의 프로세스가 도착하면 현재 실행중인 프로세스에게서 CPU를 뺏는다.
        - **비선점형** : 더 높은 우선순위의 프로세스가 도착하면 Ready Queue에 넣고 바로 다음에 실행되게 한다.
- 단점
    - Starvation
    - 무기한 봉쇄(Infinite blocking) : 우선 순위가 높은 프로세스가 Blocking 되어 있어서 CPU가 계속해서 대기해야 하는 상황
- 해결
    - Aging 프로세스의 대기 시간이 증가함에 따라 우선 순위를 높여주는 기법으로 Starvation을 예방할 수 있다.

**RR**

- 특징
    - 각 프로세스는 동일한 할당 시간 (Time Quantum)을 가진다.
    - CPU를 할당 받고 할당 시간이 지나면 Ready 상태로 돌아가 Ready Queue의 Tail로 들어간다.
    - 프로세스들이 작업을 완료할 때 까지 계속해서 순회한다.
- 장점
    - Response time이 빨라진다.
    - 모든 프로세스가 공정하게 CPU를 할당받을 수 있음을 보장한다.
- 주의할 점
    - 설정한 time quantum이 너무 커지면 FCFS와 같아진다.
    - 너무 작아지면 Context Swiitching으로 인한 Overhead가 증가한다.


> ❗**RR을 사용할 때, Time Slice에 따른 trade-off를 설명해 주세요.**


**trade-off** : 상충관계. 즉 , 하나를 얻으면 다른 하나를 잃을 수 있는 관계를 말한다. 

- **time quantum이 너무 짧으면** Context Switching이 자주 발생해 Overhead가 증가한다.
- **time quantum이 너무 길면** response time이 길어진다.


> ❗**싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?**


**우선순위 기반 스케줄링**이 적합할 거 같다. 

상시 실행되어야 하는 프로세스가 있다면 여기에 우선순위를 부여하고 , 우선순위가 높은 프로세스가 먼저 실행되게 구현해야 하지 않을까..?


> ❗**동시성과 병렬성의 차이에 대해 설명해 주세요.**


**동시성**

- 하나의 코어로 빠르게 전환하며 여러 작업을 수행하는 것
- 여러 작업이 동시에 실행되는 것처럼 보인다.

**병렬성**

- 실제로 여러 코어로 여러 작업을 동시에 수행하는 것


> ❗**타 스케쥴러와 비교하여, Multi-level Feedback Queue는 어떤 문제점들을 해결한다고 볼 수 있을까요?**


[[OS] 스케줄링: Multi-level Feedback Queue](https://velog.io/@sunkyuj/OS-%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81-Multi-level-Feedback-Queue)

1. 짧은 작업을 먼저 실행시켜 **반환 시간** 최적화
    1. SJF, STCF 알고리즘은 실행시간 정보를 필요로 하지만, 실제로 실행 시간을 미리 알 수는 없다. 
2. 대화형 사용자를 위해 **응답 시간** 최적화
    1. RR 알고리즘은 응답 시간을 최적화 하지만, 반환 시간은 거의 최악

> **프로세스에 대한 정보 없이 스케쥴링 하는 방법은 ?**
> 

MLFQ는 여러 개의 큐로 구성되어 있고, 각각 다른 **우선순위**를 배정한다. 

- 높은 우선순위 큐에 존재하는 작업부터 선택한다.

큐에는 둘 이상의 작업이 존재할 수 있고, RR 스케줄링 알고리즘이 사용된다.

- 한 큐 안의 작업들은 모두 같은 우선순위이다.

**MLFQ는 각 작업에 고정된 우선순위를 부여하는 것이 아니라 각 작업의 특성에 따라 동적으로 우선순위를 부여한다.** 

- 예를 들어, 어떤 작업이 키보드 입력을 기다리며 반복적으로 CPU를 양보 → 해당 작업에 높은 우선순위를 부여한다.
- 어떤 작업이 긴 시간 동안 CPU를 집중적으로 사용 ⇒ 해당 작업에 낮은 우선순위를 부여한다.
- 즉, MLFQ는 작업이 진행되는 동안 작업의 정보를 얻고, 이 정보를 이용하여 미래 행동을 예측한다


> ❗ **FIFO 스케쥴러는 정말 쓸모가 없는 친구일까요? 어떤 시나리오에 사용하면 좋을까요?**

FIFO는 스케쥴링을 할 때 딱히 복잡한 로직을 짤 필요가 없을 때 사용할 거 같다. 그리고 데이터나 작업이 수신되는 순서를 보장해야 할 때 사용할 수 있을 거 같다. 

> ❗**우리는 스케줄링 알고리즘을 "프로세스" 스케줄링 알고리즘이라고 부릅니다. 스레드는 다른 방식으로 스케줄링을 하나요?**


[04-5 스레드 스케줄링](https://wikidocs.net/232057)

스레드 스케줄링은 운영 체제가 프로세스 내의 스레드들 사이에서 CPU 시간을 어떻게 분배할지 결정하는 과정이다. 현대의 운영체제는 멀티 쓰레딩을 지원하며, 이는 하나의 프로세스가 동시에 여러 작업을 수행할 수 있게 해준다. 스레드 스케줄링은 이런 멀티 스레드 환경에서 매우 중요한 역할을 한다. 

스레드 스케줄링에 사용되는 알고리즘은 프로세스 스케줄링과 유사한 알고리즘을 포함할 수 있다. 커널 수준 스레드는 프로세스 스케줄링을 그대로 적용 가능하다. 

**라운드 로빈(Round Robin)**: 각 스레드에 동일한 크기의 타임 슬라이스를 할당하고, 모든 스레드에게 순차적으로 CPU 시간을 할당한다.

**우선순위 기반(Priority-Based)**: 각 스레드에 우선순위를 할당하고, 높은 우선순위의 스레드에게 먼저 CPU 시간을 할당한다.

**최소 남은 시간 우선(Shortest Remaining Time First, SRTF)**: 남은 실행 시간이 가장 짧은 스레드에게 우선적으로 CPU를 할당한다.


> ❗**유저 스레드와 커널 스레드의 스케쥴링 알고리즘은 똑같을까요?**


사용자 수준 스레드의 스케줄링은 애플리케이션에 의해 관리되고, OS는 이러한 스레드의 존재를 인식하지 못한다. 

반면, 커널 수준 스레드의 스케줄링은 운영 체제에 의해 직접 관리되고, 운영체제는 각 스레드에 CPU 시간을 할당한다. 


> 💡 **뮤텍스와 세마포어의 차이점은 무엇인가요?**

[뮤텍스(Mutex)와 세마포어(Semaphore)의 차이](https://worthpreading.tistory.com/90)

**Mutex**

key에 해당하는 어떤 오브젝트가 있으며 이 오브젝트를 소유한 것 만이 공유자원에 접근할 수 있는 것이다. 즉, 한 스레드 혹은 프로세스에 의해 소유될 수 있는 Key를 기반으로 한 상호배제기법

**Semaphore**

공통으로 관리하는 하나의 값을 이용해 상호배제를 달성하는 것이다. Signaling 메커니즘이다. 현재 공유 자원에 접근할 수 있는 스레드, 프로세스의 수를 나타내는 값을 두어 상호배제를 달성한다. 

> ❗**이진 세마포어와 뮤텍스의 차이에 대해 설명해 주세요.**

[Binary Semaphore vs Mutex](https://blog.seongjun.kr/26-difference-between-binary-semaphore-and-mutex/)

**Binary Semaphore**

- 값을 0과 1만 가지는 세마포어다
- 상호 배제를 위해 신호 전달 메커니즘을 사용해서 잠금을 구현한다.
- 세마포어가 0이면 잠겨있는 것이고, 1이면 잠금 해제 된 것이다.

**Mutex**

- 상호 배제를 Locking과 UnLocking으로 제공
- 하나의 스레드만 전체 버퍼와 함께 작업 가능

**둘의 차이는 ?**

![image](/uploads/90fe2690a46818fac19cc292ebb3068d/image.png){width=516 height=297}


> ❗**Lock을 얻기 위해 대기하는 프로세스들은 Spin Lock 기법을 사용할 수 있습니다. 이 방법의 장단점은 무엇인가요? 단점을 해결할 방법은 없을까요?**

[멀티스레드 환경에서의 락(Lock) : SpinLock, Sleep, Event](https://velog.io/@enamu/%EB%A9%80%ED%8B%B0%EC%8A%A4%EB%A0%88%EB%93%9C-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C%EC%9D%98-%EB%9D%BDLock-SpinLock-Sleep-Event)

[[OS] Spin Lock (스핀락)에 대해 알아보자](https://hogwart-scholars.tistory.com/entry/OS-Spin-Lock-%EC%8A%A4%ED%95%80%EB%9D%BD%EC%97%90-%EB%8C%80%ED%95%B4-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90)

**Spin Lock**

> Race Condition 상황에서 Lock이 반환될 때까지, 즉 Critical section에 진입 가능할 때까지 프로세스가 재시도하며 대기하는 상태
> 

락을 획득할 수 있을 때 까지 스레드가 루프를 돌며 기다리는 동기화 기법이다. 잠금을 얻을 때까지 스레드가 계속해서 CPU를 사용하므로, 짧은 기다림 시간에 효율적이다. 하지만 장시간 기다릴 경우 CPU 자원 낭비가 심해질 수 있다. 

스레드가 계속해서 CPU를 사용하므로 CPU 사용률이 높아질 수 있다는 단점이 있는데, 이를 개선하기 위해 sleep_for 및 yield 함수를 사용하여 스레드가 일정 시간 동안 대기하거나 자발적으로 CPU를 양보하는 방법을 제공할 수 있다. 이를 통해 다른 스레드가 LOCK을 얻을 수 있는 기회를 제공하고, CPU 사용률을 낮출 수 있다. 

- 정리
    
    Race Condition : 멀티 프로세스 환경에서 프로세스가 수행되는 순서에 따라 결과 값이 달라질 수 있는 상황
    
    위와 같은 경쟁 상태를 야기하는 상황을, 임계 구역에 동시에 여러 스레드가 접근한 상황이라고 정리할 수 있다. 
    
    Critical Section : 여러 스레드 또는 프로세스가 공유 자원에 접근할 수 있는 코드 영역. 즉, 코드 상에서 Race Condition가 발생할 수 있는 곳으로 둘 이상의 스레드가 동시에 접근하면 안되는 구역
    


> ❗**뮤텍스와 세마포어 모두 커널이 관리하기 때문에, Lock을 얻고 방출하는 과정에서 시스템 콜을 호출해야 합니다. 이 방법의 장단점이 있을까요? 단점을 해결할 수 있는 방법은 없을까요?**

**단점**

**오버 헤드**

시스템 콜은 사용자모드에서 커널 모드로  전환이 반복되며, 이로 인해 상당한 오버 헤드가 발생할 수 있다. 

**컨텍스트 스위칭**

락을 얻기 위해 블록 될 경우, 컨텍스트 스위칭이 발생하여 추가적인 성능 저하가 발생할 수 있다. 

**해결 방법**

**스핀 락**

- 간단한 경우에는 스핀 락을 사용하여 사용자 모드에서 락을 관리할 수 있다. 이는 시스템 콜의 오버헤드를 피할 수 있지만, 스핀락의 단점을 해결해야 한다.

**배리어 락**

- 사용자 모드에서 구현된 배리어를 사용하여 특정 조건이 만족될 때 까지 대기하는 방식으로 , 시스템 콜의 빈도를 줄일 수 있다.

**Futex**

리눅스에서 제공하며 사용자 모드에서 빠르게 락을 획득하고 해제할 수 있도록 설계된 메커니즘이다.

- Futex는 락이 경쟁상태가 없을 때는 사용자 모드에서 처리하고, 있을 때만 커널 모드로 전환하여 처리하는데, 이는 시스템 콜의 오버헤드를 크게 줄인다.

**락 분할**

락을 세분화하여 여러 락으로 나누어 경합을 줄인다.

- 예를 들어, 데이터 구조의 각 부분에 대해 별도의 락을 사용하는 방식이 있다.

**락 스트리핑**

해시 테이블 등의 구조에서 각 버킷에 별도의 락을 적용하여 경합을 줄인다.


# 0123 TIL
## OS

> **Deadlock 에 대해 설명해 주세요.**

두 개 이상의 프로세스나 스레드가 서로 자원을 얻지 못해서 다음 일을 수행하지 못하는 상태이다. 무한히 다음 자원을 기다리는 상태를 말한다. 시스템적으로 한정된 자원을 여러 곳에서 사용하려고 할 때 발생한다. 


> ❗ **Deadlock 이 동작하기 위한 4가지 조건에 대해 설명해 주세요.**

**상호 배제**

자원은 한 번에 한 프로세스 만이 사용할 수 있어야 한다,

**점유 대기**

최소한 하나의 자원을 점유하고 있으면서 다른 프로세스에 할당되어 사용하고 있는 자원을 추가로 점유하기 위해 대기하는 프로세스가 있어야 한다. 

**비선점**

다른 프로세스에 할당된 자원은 사용이 끝날 때 까지 강제로 빼앗을 수 없어야 한다.

**순환 대기**

프로세스의 집합에서 P0는 P1이 점유한 자원을 대기하고 P1는 P2가 점유한 자원을 대기하고 P2…Pn-1은 Pn이 점유한 자원을 대기하며 Pn은 P0가 점유한 자원을 요구해야 한다.


> ❗ **그렇다면 3가지만 충족하면 왜 Deadlock 이 발생하지 않을까요?**


[[OS] 교착 상태의 해결 방법](https://s-y-130.tistory.com/329)

쉽게 말하면 프로세스나 스레드가 언젠가는 자원을 얻을 수 있기 때문이다. 

Deadlock 발생에 필요한 조건 중 “순환 대기” 조건이 포함되어 있기 때문이다.

순환 대기로 예를 들면,

- 상호 배제 조건이 없으면 두 개 이상의 프로세스가 자원을 동시에 사용할 수 있으므로, 대기할 필요가 없어진다.
- 점유 및 대기 조건이 없으면 자원을 요청한 프로세스는 해당 자원을 할당받지 못하고 바로 실패하게 되므로 대기할 필요가 없어진다.
- 비선점 조건이 없으면 다른 프로세스가 자원을 강제로 해제할 수 있으므로 프로세스들이 서로 계속해서 자원을 해제하고 점유할 수 있게 된다.

따라서, 세 가지 조건만 충족된다면 순환 대기 조건이 없어서 Deadlock이 발생하지 않으며, 순환 대기 조건이 포함된 경우 4가지 조건이 모두 충족되어야 Deadlock이 발생한다.


> ❗ **어떤 방식으로 예방할 수 있을까요?**


데드락의 발생 조건 4가지 중 하나라도 발생하지 않게 한다.

- **자원의 상호 배제 조건 방지**
    - 한 번에 여러 프로세스가 공유 자원을 사용할 수 있게 한다. 그러나 추후 동기화 문제가 발생할 수는 있다.
- **점유 대기 조건 방지**
    - 프로세스 실행에 필요한 모든 자원을 한꺼번에 요구하고 허용할 때 까지  작업을 보류해서, 나중에 또 다른 자원을 점유하기 위한 대기 조건을 성립하지 않도록 한다.
- **비선점 조건 방지**
    - 이미 다른 프로세스에게 할당된 자원이 선점권이 없다고 가정할 떄, 높은 우선순위의 프로세스가 해당 자원을 선점 할 수 있도록 한다.
- **순환 대기 조건 방지**
    - 자원을 순환 형태로 대기하지 않도록 일정한 한 쪽 방향으로 자원을 요구할 수 있도록 한다.


> ❗ **왜 현대 OS는 Deadlock을 처리하지 않을까요?**

[[OS] DeadLock(데드락)](https://peonyf.tistory.com/entry/OS-DeadLock%EB%8D%B0%EB%93%9C%EB%9D%BD)

1. 빈번히 발생하는 이벤트가 아니기 때문에 미연에 방지하기 위해 훨씬 더 많은 오버헤드를 들이는 것이 비효율적이라고 판단하기 때문이다. 
2. 현대 시스템의 복잡성으로 인해 교착 상태를 완전히 방지하는 것은 불가능하기 대문이다.
3. 만약 시스템에서 deadlock이 발생한 경우 시스템이 비정상적으로 작동한 것을 사람이 느낀 후 직접 process를 죽이는 방법으로 대처한다. 


> ❗ **Wait Free와 Lock Free를 비교해 주세요.**

[[OS] DeadLock(데드락)](https://peonyf.tistory.com/entry/OS-DeadLock%EB%8D%B0%EB%93%9C%EB%9D%BD)

Wait Free와 Lock Free는 모두 멀티스레딩 환경에서 공유 자원에 대한 동시 접근을 처리하는 기술이다. Wait free는 모든 스레드가 동시에 진행될 수 있는 것에 비해, Lock-free는 일부 스레드가 블로킹 될 가능성이 있지만, 구현이 더 쉽고 성능이 높은 경우가 많다. 

![image](/uploads/4b3d36711c7a4865a71194f5c5914874/image.png){width=661 height=417}

- 자세히
    
    [Lock-free VS Wait-free](https://chfhrqnfrhc.tistory.com/entry/Lockfree-VS-Waitfree)
    
    Lock-Free와 Wait-Free의 상관 관계를 보면 포함 관계다. Lock-Free가 Wait-free를 포함하고 있다. 즉, 모든 Wait-Free는 Lock-Free이다. 
    
    **Lock-Free Algorithm**
    
    여러개의 쓰레드에서 동시에 호출했을 때에도 정해진 단위시간 마다 적어도 한 개의 호출이 완료되는 알고리즘을 말한다. 
    
    - 멀티 스레드에서 동시에 호출해도 정확한 결과를 보증함
    - Non-Blocking으로 동작한다.
    - Lock을 사용하지 않아야한다.
    - 호출이 다른 스레드와 충돌하였을 경우, 적어도 하나의 승자가 있으며 그 승자 스레드는 딜레이 없이 완료된다.
    
    **Wait-Free Alogrithm**
    
    위의 개념에서 **다른 스레드의 방해가 없다면이라는 말이 해결된 알고리즘**이다. 결과적으로 더 좋은 알고리즘이라고 할 수 있다. 
    
    일단, Lock Free와는 포함 관계에 있으므로 모든 Wait-Free는 Lock-Free라고 볼 수 있다. 따라서 Lock-Free가 가지는 모든 조건을 만족해야 하며, 여기에 **추가적으로 다른 스레드와 충돌해도 모두 Delay 없이 완료되어야 한다**는 조건이 붙는다. 
    
    Lock-Free가 다른 스레드에 의해 방해 받아 기아 상태가 발생하면, Wait-Free는 상수번 루프를 돌면 반드시 루프를 종료해야 한다.



## CKA
### Service

**서비스(Service)**는 **파드들을 통해 실행되고 있는 애플리케이션을 네트워크에 노출(expose)시키는 가상의 컴포넌트**

쿠버네티스의 서비스는 앱 안팎의 다양한 구성 요소 간의 통신을 가능하게 한다. 

쿠버네티스 서비스는 애플리케이션을 다른 애플리케이션 또는 사용자와 연결하는 것을 도와준다.

가령, 우리 응용 프로그램에는 다양한 섹션을 실행하는 포드가 여럿 있다. 사용자에게 프론트 엔드 로드를 제공하기 위한 그룹 , 백엔드 프로세스 실행을 위한 다른 그룹, 외부 데이터 소스에 연결하는 세번째 그룹 같은 것이 있다.

이런 포드 그룹 간의 연결을 가능하게 하는 것이 서비스 이다. 

서비스는 프론트엔드 애플리케이션을  최종 사용자가 사용할 수 있게 한다.

백엔드와 프론트앤드 포드의 통신과 외부 데이터 소스와의 연결을 돕는다. 이렇게 서비스는 두 가지의 미세한 서비스를 쉽게 연결할 수 있게 한다. 

![image](/uploads/b43c50de742b801ff38cf5505c93ca2b/image.png){width=560 height=278}

쿠버네티스 노드에 192.168.1.2 ip 주소가 있고, 사용자 노트북도 같은 네트워크라서 192.168.1.10 이라는 ip가 있다. 내부 포드 네트워크가 10구역에 있는데, 10.244.0.0이 있고 포드는 10.244.0.2 가 있다. 

원래는 네트워크가 달라서 포드에 접속할 수 없어야 하는데, 이를 가능하게 하는 것은 무엇인지 보자.

192에서 쿠버네티스 노드로 curl을 줘서 포드의 웹 페이지에 접속이 가능하다. 하지만 이건 쿠버네티스 노드 내부에서 온거고 우리가 원하는 것은 그게 아니다. SSH를 누르지 않고 사용자의 노트북에서 웹 서버에 액세스 하고 싶다. 쿠버네티스의 노드에 단순히 액세스 해서.

- SSH
    
    SSH란 Secure SHell의 약자로써 **네트워크상의 다른 컴퓨터에 로그인하여 명령을 실행하고 정보를 보고 받을수 있도록 해 주는 통신 프로토콜**
    
- curl
    
    커멘드 모드로 url을 호출해서 결과를 보고 싶을 때 사용한다. 
    

따라서 우리는 중간에 **노트북에서 노드로의 요청을 매핑하기 위한 중간 무엇인가가 필요**하다. 노드를 통해 웹 컨테이너를 실행중인 포드를 매핑해야 한다. 여기서 **서비스**의 개념이 나온다. 

## Services Types

### Node Port

**NodePort**는 **외부에서 노드 IP의 특정 포트(`<NodeIP>:<NodePort>`)로 들어오는 요청을 감지하여, 해당 포트와 연결된 파드로 트래픽을 전달**하는 유형의 서비스다. 

### ClusterIP

**ClusterIP**는 **파드들이 클러스터 내부의 다른 리소스들과 통신**할 수 있도록 해주는 가상의 클러스터 전용 IP다. 즉, 클러스터 내부에서 가상 IP를 만들어 다른 서비스 간의 통신을 가능하게 한다. 프론트엔드 서버 모음과 백 엔드 서버 모음이 그 예시이다. 

### Load Balancer

별도의 외부 로드 밸런서를 제공하는 클라우드*(AWS, Azure, GCP 등)* 환경을 고려하여, 해당 로드 밸런서를 클러스터의 서비스로 프로비저닝할 수 있는 **LoadBalancer** 유형도 제공된다. 이 유형은 **서비스를 클라우드 제공자 측의 자체 로드 밸런서로 노출**시킨다. 

좋은 예시로는 프론트 엔드 계층의 다양한 웹 서버에 로드 배포를 하는 것이다. 

## Node Port

외부에서 노드 IP의 특정 포트로 들어오는 요청을 감지하여, 해당 포트와 연결된 파드로 트래픽을 전달하는 유형의 서비스이다. 

![image](/uploads/7e4268d810178301797f3a7f5e435024/image.png){width=631 height=320}

**타깃 포트**는 80이다. 서비스가 요청을 전달하는 곳이기 때문이다.

두 번째는 **서비스 포트**가 있다. 이는 간단하게 Port 라고 불린다. 서비스는 노드 안의 가상 서버 같은 것이다. 클러스터 내부에서 그것은 고유한 IP 주소를 갖는다. 이 IP 주소를 Cluster IP 라고 한다. 

마지막으로는 **Node Port**가 있다. 이는 외부에서 웹 서버에 엑세스 하는데 사용하는 것이다. Node Port의 경우 30000 - 32767의 유효한 범위 내에만 있을 수 있다. 

## Service 생성하기

![image](/uploads/4d46bce75277fe98190d799a7cbd5e03/image.png){width=652 height=345}

NodePort는 유효 범위 내의 어떤 숫자든 설정한다. 

위 중에서 필수 필드는 port 뿐이다. target port를 설정하지 않으면 port와 같다고 여겨진다. Node Port를 제공하지 않으면 유효한 범위의 자유 포트가 자동으로 할당 된다. 

ports는 배열이라는 것을 주목하자. 포트 섹션 밑에 - 가 있다. 이는 첫 번째 요소를 나타낸다. 

하나의 서비스 내에서는 여러 개의 포트 매핑을 가질 수 있다. 

위에서 우리는 목표지점이 어느 포드인지는 언급하지 않았다. 포트 80에서 웹 서비스를 실행하는 포드는 수백 개 일 수 있다. 이럴 땐 어떻게 해야 할까?

![image](/uploads/e0b9641db6e8825a48db294ce3ef7942/image.png){width=671 height=321}

selector을 이용해서 이 것들을 연결한다. pod에 라벨이 붙었으므로, 라벨을 서비스 정의 파일에 가져와야 한다. 

spec 섹션에 selector 속션이 있다. pod 정의 파일에서 meta data 아래 있는 labels을 떼서 (예를 들어 app과 type) selector 아래에 두면 된다. 이렇게 하면 서비스와 포드가 연결된다. 

이제 우리는 curl이나 웹 브라우저를 통해 웹 서비스에 접속할 수 있다. 

**포드가 여러개면 어떻게 해야 할까 ?**

![image](/uploads/2ae590f8da5b7afecde148fa1a81dccf/image.png){width=624 height=304}

위 3개는 모드 같은 labels를 같는다. 따라서 같은 lables이 서비스 생성 시 selector로 사용된다. 서비스가 생성되면 lables이 있는 포드를 찾아서 총 3개를 찾고, 서비스는 자동으로 끝 점인 세 개의 포드를 선택해 사용자가 보내는 외부 요청을 전달한다. 이 것을 가능하게 하는데에는 추가적인 구성이 필요 없다. 

이 때, 클러스터 내부로 들어온 트래픽을 특정 파드로 연결하기 위한 Cluster IP 역시 자동으로 생성된다. 

**마지막으로, 포드가 여러 노드에 분산되면 어떻게 될까?**

![image](/uploads/61d270ee8b185cd1accfd1eebb6a2489/image.png){width=666 height=360}

이 경우 클러스터 내 분리된 노드의 포드에 웹 응용 프로그램이 있다. 이 또한 추가적인 구성을 할 필요 없이 서비스를 생성할 때 쿠버네티스는 자동으로 클러스터 내 모든 노드에 걸쳐 서비스를 생성하고 타깃 포트를 같은 노드 포트로 매핑한다. 클러스터 내의 모든 노드에 말이다. 

## Services Cluster IP

pod 들이 클러스터 내부의 다른 리소스들과 통신할 수 있도록 해주는 가상의 cluster 전용 ip이다. 이 경우 오직 클러스터 내부에서만 접근 가능하게 된다. 쿠버네티스가 지원하는 기본적인 형태의 서비스 이다. 


풀 스택 웹 응용 프로그램은 응용 프로그램의 다른 부분을 호스팅 하는 다른 종류의 포드가 있다. 프론트엔드 웹 서버를 실행하는 포드도 있고 백 엔드 서버를 실행하는 포드도 있고 Redis 같은 키 값 저장소를 실행하는 포드도 있고 등등이 있다. 

웹 프론트엔드 서버는 백엔드 서버와 통신하기를 원하고 백엔드 서버는 데베나 Redis와 통신하길 원한다.

**앱의 이런 서비스나 계층 간의 연결을 확립하는 올바른 방법은 무엇일까?**

pod는 모두 할당된 IP 주소가 있다. 하지만 이 IP들은 정적인 것이 아니다. 해당 포드들은 언제든 철거될 수 있고, 새 포드는 계속 만들어 지니까 앱 간의 내부 통신에 이 IP 주소에만 의존할 수는 없는 것이다. 

예를들어 프론트 엔드의 10.244.0.3이 백 엔드 서비스에 접속해야 한다고 하면, 3개 중 누구에게 줄 것이며 누가 결정을 내릴까? 

쿠버네티스의 **서비스를 통해 포드를 하나로 묶고 하나의 인터페이스를 통해 단체 포드에 접속할 수 있다.** 

예를 들어 백 엔드 포드를 통해 만든 서비스는 모든 백엔드 포드를 하나로 묶어서 다른 포드가 이 서비스에 액세스 할 수 있는 단일 인터페이스를 제공한다. 요청은 무작위로 한 포드에 전달된다. 

이런 것들을 통해 쿠버네티스 클러스터에 MSA를 쉽고 효과적으로 배포할 수 있는 것이다. 

각 계층은 필요한 대로 확장하거나 이동할 수 있다. 

**각각의 서비스는 클러스터 내부에서 IP와 그에 할당된 name을 가지고 있다. 다른 포드가 서비스에 접근할 때 해당 Name을 사용해서 접근**해야 한다. 이런 서비스 유형이 **Cluster IP** 이다. 

## Cluster IP 만들기

![image](/uploads/afb0aeaf26a07a6d61f175ed0887240f/image.png){width=678 height=313}

사실 Cluster IP는 기본 형식이라 지정하지 않아도 자동으로 Cluster IP로 추정한다. 

ports 아래에는 targetPort와 port가 있다. targetPort는 애플리케이션(파드, 여기선 backend)를 노출하는 포트이고, port는 서비스를 노출하는 포트이다. 

selector은 이 서비스가 적용될 pod 정보를 지정한다. ( pod 정의 파일을 참조한 다음 라벨을 복사하고, selector 아래 이동 시킴)

## Load Balancer

최종 사용자는 클러스터 내부의 모든 노드 포트에 연결이 가능하다, 하지만 최종 사용자는 단일 URL로 접근하고 싶다. 이렇게 하는 방법은 부하 분산 목적을 위한 새로운 VM을 생성하고 적절한 Load Balancer을 설치하고 구성하는 것이 있다. Nginx 같은 것이 있다. 그런 다음 기본 노드로 로드 밸런서 루트 트래픽을 구성한다. 외부 부하 분산을 설정하고 유지 관리를 하는 것은 귀찮은 작업이다.

하지만 우리가 구글 클라우드나 AWS와 같은 지원된 클라우드 플랫폼을 사용한다면, 클라우드 플랫폼의 기본 로드 밸런스 기능을 활용할 수 있다. 쿠버네티스는 이러한 기본 로드 밸런서와 통합할 수 있도록 지원한다. 특정 클라우드 제공자가 그것을 구성한다.

우리가 할 일은 서비스 유형을 프론트엔드 서비스에 맞춰서 로드 밸런서를 설정하는 것이다. 이건 지원되는 클라우드 플랫폼만 작동한다. (구글 클라우드 플랫포므 AWS, Azure 등)
